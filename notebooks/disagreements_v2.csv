,nb_link,nb_id,cell_no,source_x,Mike_label_readable,Ge_label_readable
1602,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1000752.ipynb,1000752,6,['p =  figure()'],EXPLORE,WRANGLE
1609,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1000752.ipynb,1000752,13,"['gr  =  Bar(data, title = ""Gráfico Bokeh"", xlabel = \'x\', ylabel = \'valores\', width = 400, height = 400)']",EXPLORE,WRANGLE
1738,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1002722.ipynb,1002722,1,"[""with open('/home/czc/Documents/Data/phone_data/phone_data/tb_sms_201202.txt') as f:\n"", '    dat = f.readlines() \n', '\n', 'G = nx.DiGraph()\n', 'for i in dat:\n', '    date, x, y = i.split(""\\t"")[:3]\n', ""    if 'y' not in x and 'y' not in y:\n"", '        G.add_edge(x,y)\n', '\n', 'components = list(nx.weakly_connected_components(G))\n', '\n', 'clen_dict = defaultdict(int)\n', 'for i in components:\n', '    clen_dict[len(i)] += 1\n', '    \n', 'scc = max(nx.algorithms.components.strongly_connected.strongly_connected_components(G), key=len)\n', '\n', 'bfsnodes = nx.bfs_successors(G, scc[0])\n', 'rG = G.reverse()\n', 'rbfsnodes = nx.bfs_successors(rG, scc[0])\n', '\n', 'bfsnodeslist= []\n', 'for i in bfsnodes.keys():\n', '    bfsnodeslist.append(i)\n', 'for i in bfsnodes.values():\n', '    for j in i:\n', '        bfsnodeslist.append(j)\n', 'bfsnodesset = set(bfsnodeslist)\n', '\n', 'rbfsnodeslist= []\n', 'for i in rbfsnodes.keys():\n', '    rbfsnodeslist.append(i)\n', 'for i in rbfsnodes.values():\n', '    for j in i:\n', '        rbfsnodeslist.append(j)\n', 'rbfsnodesset = set(rbfsnodeslist)']",WRANGLE,MODEL
63,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1004385.ipynb,1004385,2,"[""# Let's use pandas to create Categorical Series. One way is by \n"", '# specifying dtype=""category"" when constructing a Series:\n', '\n', 's = pd.Series([""a"",""b"",""c"",""a""], dtype=""category"")\n', 's']",WRANGLE,EXPLORE
64,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1004385.ipynb,1004385,3,"['# Another way is to convert an existing Series or column to a \n', '# category dtype:\n', '\n', 'df = pd.DataFrame({""A"":[""a"",""b"",""c"",""a""]})\n', 'df[""B""] = df[""A""].astype(\'category\')\n', 'df']",WRANGLE,EXPLORE
66,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1004385.ipynb,1004385,5,"[' s = pd.Series(raw_cat)\n', ' s']",WRANGLE,EXPLORE
68,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1004385.ipynb,1004385,8,"[""# Now, let's convert the categorical variables into dummy variables. \n"", '\n', ""pd.get_dummies(df['key'])""]",WRANGLE,EXPLORE
882,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005070.ipynb,1005070,24,"['pcbp2 = ReadDensity(pos=""/projects/ps-yeolab2/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_1120_RBFOX2.merged.r2.norm.neg.bw"", \n', '                    neg=""/projects/ps-yeolab2/encode/analysis/encode_v12/293XT_CLIP_RBFOX2_1120_RBFOX2.merged.r2.norm.pos.bw"")\n', 'miso_splicing_calls = pd.read_table(""/home/elvannostrand/scratch/ENCODE_rnaseq/bam_files_shashank/HepG2/clip_and_RNASeq_common_datasets/miso/miso_comparisons/xrcc6_rep1/SE_vs_SE/bayes-factors/SE_vs_SE.miso_bf"")\n', '\n', 'plot_splice_map(pcbp2, se_splicing, os.path.join(img_dir, ""RBFOX2.splice_map.svg""))']",EXPLORE,WRANGLE
883,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005070.ipynb,1005070,25,"['pcbp2 = ReadDensity(pos=""/projects/ps-yeolab2/encode/analysis/encode_v12/204_02_RBFOX2.merged.r2.norm.neg.bw"", \n', '                    neg=""/projects/ps-yeolab2/encode/analysis/encode_v12/204_02_RBFOX2.merged.r2.norm.pos.bw"")\n', 'miso_splicing_calls = pd.read_table(""/oasis/tscc/scratch/elvannostrand/ENCODE_rnaseq/bam_files_shashank/K562/clip_and_RNASeq_common_datasets/miso/miso_comparisons/RBFOX2_rep1_ctrl1/SE_vs_SE/bayes-factors/SE_vs_SE.miso_bf"")\n', '\n', 'plot_splice_map(pcbp2, miso_splicing_calls, ""204_02_RBFOX2.splice_map.svg"")']",EXPLORE,WRANGLE
887,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005070.ipynb,1005070,31,"['def modify_plot(df):\n', '    #df = df[df.sum(axis=1) > 5]\n', '    min_normalized_read_number = min([item for item in df.unstack().values if item > 0])\n', '    df = df + min_normalized_read_number\n', '    df = df.div(df.sum(axis=1), axis=0).dropna()\n', '    return df.mean()\n', '    return df']",EXPLORE,WRANGLE
910,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005070.ipynb,1005070,56,['cryptic_binding_counts.unstack()'],WRANGLE,EXPLORE
911,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005070.ipynb,1005070,58,['HTML(cryptic_events_slop_counts.unstack().to_html())'],EXPLORE,WRANGLE
916,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005070.ipynb,1005070,65,"['for (uid, rbp, cell_type), df in list(sig_rMATS.groupby(level=[""uid"", ""rbp"", ""cell_type""])):\n', '    try:\n', '        read_density = splicing_map.ReadDensity(pos=""/projects/ps-yeolab2/encode/analysis/encode_v12/{}_01_{}.merged.r2.norm.neg.bw"".format(uid, rbp), \n', '                                   neg=""/projects/ps-yeolab2/encode/analysis/encode_v12/{}_01_{}.merged.r2.norm.pos.bw"".format(uid, rbp))\n', '\n', '        splicing_map.plot_splice_map(read_density, df, ""{} {} Rep 1"".format(rbp, cell_type),  os.path.join(img_dir, ""{}_01_{}.splice_map.svg"".format(uid, rbp)))\n', '\n', '        read_density = splicing_map.ReadDensity(pos=""/projects/ps-yeolab2/encode/analysis/encode_v12/{}_02_{}.merged.r2.norm.neg.bw"".format(uid, rbp), \n', '                                   neg=""/projects/ps-yeolab2/encode/analysis/encode_v12/{}_02_{}.merged.r2.norm.pos.bw"".format(uid, rbp))\n', '\n', '        splicing_map.plot_splice_map(read_density, df, ""{} {} Rep 2"".format(rbp, cell_type),  os.path.join(img_dir, ""{}_02_{}.splice_map.svg"".format(uid, rbp)))\n', '        \n', '    except Exception as e:\n', '        print e\n', '        print uid, rbp, cell_type']",EXPLORE,WRANGLE
1769,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005207.ipynb,1005207,30,"['num_good_feat = len(base_features)\n', 'fitted_vs_ref_plot(df_cv_lin_base, num_good_feat, ref_column)\n', 'plot_fitted_and_ref_vs_time(df_cv_lin_base, pod_number, 1, ref_column)\n', 'plot_fitted_and_ref_vs_time(df_cv_lin_base, pod_number, 2, ref_column)\n', 'plot_fitted_and_ref_vs_time(df_cv_lin_base, pod_number, 3, ref_column)\n', 'resid = plot_resid_vs_conc(df_cv_lin_base, ref_column)']",EXPLORE,EVALUATE
1770,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005207.ipynb,1005207,32,"['every_feature = list(df_tr.ix[:,0:len(df_scaled.columns)])\n', ""leave_out = ['ref_o3_smooth', 'chunk', 'day', 'O3_ppb', 'UnixTime']\n"", 'all_features = [f for f in every_feature if f not in leave_out]\n', 'MSE_CV, MSE_T, MSE_H, high_MSE_cv, X_pred_cv_all, y_cv, df_cv_lin_all, df_H_lin_all = cross_validation_by_day(lin_regr, all_features, df_tr, df_hold, days_tr, ref_column, cutoff_value)']",WRANGLE,MODEL
1775,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005207.ipynb,1005207,42,"['i = 1\n', 'while i < 15:\n', '    num_good_feat = i\n', '    df_cv_1, df_H_1 = find_fitted_cv_values_for_best_features(df_tr, df_hold, fs_features, num_good_feat, linear_model.LinearRegression(), chunks_tr, ref_column)\n', '    fitted_vs_ref_plot(df_cv_1, i, ref_column) \n', '    plot_learning_curve(lin_regr, ""Learning Curve- Number of features = "" + str(i), df_tr[fs_features[:i]].values, df_tr[ref_column].values, (0,10), 5, np.array([0.1, 0.3, 0.5, 0.7, 0.8, 0.85, 0.9, 0.95,0.97, 1.0]))\n', '    i += 2']",EVALUATE,MODEL
1779,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1005207.ipynb,1005207,49,"['df_lin_regr_best_feat_cv, df_lin_regr_best_feat_H  = find_fitted_cv_values_for_best_features(df_tr, df_hold, fs_features, num_good_feat, linear_model.LinearRegression(), chunks_tr, ref_column)\n', 'fitted_vs_ref_plot(df_lin_regr_best_feat_cv, num_good_feat, ref_column)\n', 'plot_fitted_and_ref_vs_time(df_lin_regr_best_feat_cv, pod_number, 1, ref_column)\n', 'plot_fitted_and_ref_vs_time(df_lin_regr_best_feat_cv, pod_number, 2, ref_column)\n', 'plot_fitted_and_ref_vs_time(df_lin_regr_best_feat_cv, pod_number, 3, ref_column)\n', 'resid = plot_resid_vs_conc(df_lin_regr_best_feat_cv, ref_column)']",EVALUATE,MODEL
1040,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1006895.ipynb,1006895,18,"['#Now input into the nfl DataFrame\n', ""nfl_frame['Stadium']=stadiums\n"", '\n', '#Show\n', 'nfl_frame']",WRANGLE,EXPLORE
1041,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1006895.ipynb,1006895,19,"['#We can also delete columns\n', ""del nfl_frame['Stadium']\n"", '\n', 'nfl_frame']",WRANGLE,EXPLORE
1042,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1006895.ipynb,1006895,20,"['#DataFrames can be constructed many ways. Another way is from a dictionary of equal length lists\n', ""data = {'City':['SF','LA','NYC'],\n"", ""        'Population':[837000,3880000,8400000]}\n"", '\n', 'city_frame = DataFrame(data)\n', '\n', '#Show\n', 'city_frame']",WRANGLE,EXPLORE
457,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1024739.ipynb,1024739,3,"['import matplotlib.pyplot as plt\n', 'from iotdata import IOTData\n', '\n', 'device = IOTData()\n', '\n', 'def plotarray(data):\n', '    length = len(data)\n', '    for k in range(length):\n', '        plt.plot(range(len(data[k])), data[k])\n']",IMPORT,EXPLORE
1080,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026329.ipynb,1026329,52,"[""jieba.analyse.extract_tags(list(df['status_message'])[99], topK=120)""]",EXPLORE,MODEL
1081,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026329.ipynb,1026329,54,"['def jieba_extract(message_list):\n', '    word_count = {}\n', '    for message in message_list:\n', '        # 在抽取關鍵字時,可能會發生錯誤,先把錯誤的message收集起來,看看是怎麼一回事\n', '        seg_list = jieba.analyse.extract_tags(message, topK=120)\n', '        for seg in seg_list:\n', '            if not seg in word_count:\n', '                word_count[seg] = 1\n', '            else:\n', '                word_count[seg] += 1\n', '\n', '    sorted_word_count = sorted(word_count.items(), key=operator.itemgetter(1))\n', '    sorted_word_count.reverse()\n', '    return sorted_word_count\n', ""sorted_word_count = jieba_extract(list(df['status_message']))""]",WRANGLE,MODEL
1083,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026329.ipynb,1026329,58,"[""tpath = '/home/wy/font/NotoSansCJKtc-Black.otf'\n"", 'wordcloud = WordCloud(max_font_size=120, relative_scaling=.1, width=900, height=600, font_path=tpath).fit_words(sorted_word_count)\n', 'plt.imshow(wordcloud)\n', 'plt.axis(""off"")\n', 'plt.show()']",EXPLORE,EVALUATE
1084,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026329.ipynb,1026329,60,"[""tpath = '/home/wy/font/NotoSansCJKtc-Black.otf'\n"", 'wordcloud = WordCloud(max_font_size=120, relative_scaling=.1, width=900, height=600, font_path=tpath).fit_words(sorted_word_count[30:])\n', 'plt.imshow(wordcloud)\n', 'plt.axis(""off"")\n', 'plt.show()']",EXPLORE,EVALUATE
1088,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026329.ipynb,1026329,65,"[""sorted_comment_message = jieba_extract(list(c_df['comment_message']))\n"", 'print (sorted_comment_message[:10])\n', ""tpath = '/home/wy/font/NotoSansCJKtc-Black.otf'\n"", 'wordcloud = WordCloud(max_font_size=120, relative_scaling=.1, width=900, height=600, font_path=tpath).fit_words(sorted_comment_message)\n', 'plt.figure()\n', 'plt.imshow(wordcloud)\n', 'plt.axis(""off"")\n', 'plt.show()']",EXPLORE,MODEL
1094,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026329.ipynb,1026329,77,"['# 設定路徑\n', ""excel_path = 'excel/'+page_id+'_analysis.xlsx'\n"", ""writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')\n"", '\n', '# 把DataFrame寫入到xlsx\n', 'df_num_reactions.to_excel(writer, sheet_name=page_id, startcol=0, startrow=0)\n', 'df_num_comments.to_excel(writer, sheet_name=page_id, startcol=3, startrow=0)\n', 'df_num_shares.to_excel(writer, sheet_name=page_id, startcol=6, startrow=0)\n', 'delta_datetime_df.to_excel(writer, sheet_name=page_id, startcol=9, startrow=0)\n', '\n', 'df_status_type.to_excel(writer, sheet_name=page_id, startcol=0, startrow=11)\n', ""df_weekday.set_index('index').to_excel(writer, sheet_name=page_id, startcol=0, startrow=25)\n"", ""df_hour.set_index('index').to_excel(writer, sheet_name=page_id, startcol=0, startrow=39)\n"", '\n', '\n', '# 畫出內建長條圖\n', 'workbook  = writer.book\n', '\n', '# 發文種類長條統計圖\n', ""chart1 = workbook.add_chart({'type': 'column'})\n"", 'chart1.add_series({\n', ""    'categories': '='+page_id+'!$A$13:$A$18',\n"", ""    'values': '='+page_id+'!$B$13:$B$18',\n"", '})\n', ""chart1.set_title ({'name': '發文種類長條統計圖'})\n"", ""chart1.set_x_axis({'name': 'status_type'})\n"", ""chart1.set_y_axis({'name': 'count'})\n"", 'worksheet = writer.sheets[page_id]\n', ""worksheet.insert_chart('D12', chart1)\n"", '\n', '# 星期幾發文統計長條圖\n', ""chart2 = workbook.add_chart({'type': 'column'})\n"", 'chart2.add_series({\n', ""    'categories': '='+page_id+'!$A$27:$A$33',\n"", ""    'values': '='+page_id+'!$B$27:$B$33',\n"", '})\n', ""chart2.set_title ({'name': '星期幾發文統計長條圖'})\n"", ""chart2.set_x_axis({'name': 'hour'})\n"", ""chart2.set_y_axis({'name': 'count'})\n"", 'worksheet = writer.sheets[page_id]\n', ""worksheet.insert_chart('D26', chart2)\n"", '\n', '# 單日幾時發文統計長條圖\n', ""chart3 = workbook.add_chart({'type': 'column'})\n"", 'chart3.add_series({\n', ""    'categories': '='+page_id+'!$A$41:$A$64',\n"", ""    'values': '='+page_id+'!$B$41:$B$64',\n"", '})\n', ""chart3.set_title ({'name': '單日幾時發文統計長條圖'})\n"", ""chart3.set_x_axis({'name': 'weekday'})\n"", ""chart3.set_y_axis({'name': 'count'})\n"", 'worksheet = writer.sheets[page_id]\n', ""worksheet.insert_chart('D40', chart3)\n"", '\n', '# 示範插入image, 當把上面的圖畫出來之後,要先存起來才能插入到xlsx\n', ""df.plot(x='datetime', y=['num_likes', 'num_loves', 'num_wows', 'num_hahas', 'num_sads', 'num_angrys'])\n"", ""plt.savefig('image/image1.png')\n"", ""worksheet.insert_image('L12', 'image/image1.png')""]",WRANGLE,EXPLORE
927,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026834.ipynb,1026834,10,"['import numpy\n', 'type(numpy.nan)']",EXPLORE,IMPORT
931,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026834.ipynb,1026834,14,"['import numpy\n', 'type(numpy.nan)']",EXPLORE,IMPORT
940,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026834.ipynb,1026834,24,"['import numpy\n', 'columns_that_are_numbers = billboard.columns[\n', '(billboard.dtypes == numpy.int64) | (billboard.dtypes == numpy.float64)]\n', 'columns_that_are_numbers ']",EXPLORE,WRANGLE
946,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1026834.ipynb,1026834,30,"[""genre_table = pandas.pivot_table(billboard,index='genre').T\n"", 'genre_table']",EXPLORE,WRANGLE
260,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1031433.ipynb,1031433,26,['print x_min.x'],EVALUATE,EXPLORE
264,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1031433.ipynb,1031433,30,"['X  = np.random.rand(4,3) \n', 'print X']",EXPLORE,WRANGLE
265,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1031433.ipynb,1031433,31,"['U,D,V = linalg.svd(X)\n', 'print U.shape , D.shape, V.shape\n', 'print type(U), type(D), type(V)']",EXPLORE,MODEL
271,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1031433.ipynb,1031433,39,"[""f = interpolate.interp1d(x,y, kind = 'quadratic')\n"", 'x_new = np.arange(0,8,0.1)\n', 'y_new = f(x_new)\n', 'print y_new\n', 'print x_new']",WRANGLE,MODEL
26,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1033591.ipynb,1033591,5,"['# Predicted outcomes\n', 'predicted = model.predict(X_test)\n', '\n', '# Actual Expected Outvomes\n', 'expected = Y_test']",MODEL,EVALUATE
964,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1034447.ipynb,1034447,0,"['#第一题binomial distribution (Math question没做)\n', '\n', 'import numpy as np\n', 'from sklearn.tree import DecisionTreeRegressor\n', 'from sklearn.ensemble import BaggingRegressor\n', 'import matplotlib.pyplot as plt\n', '# Create a random dataset\n', 'rng = np.random.RandomState(1)\n', 'X = np.sort(5 * rng.rand(80, 1), axis=0)\n', 'y = np.sin(X).ravel()\n', 'y[::5] += 1 * (0.5 - rng.rand(16))\n', 'n_estimators = 10 # L in the text\n', 'tree_max_depth = 10\n', 'bagging_max_depth = 10\n', '\n', '# TODO define the regressor by bagging stumps\n', 'tree = DecisionTreeRegressor(max_depth=1)\n', 'tree.fit(X, y)\n', 'tree_ = DecisionTreeRegressor(max_depth=10)\n', 'tree_.fit(X, y)\n', '\n', 'bagging = BaggingRegressor(n_estimators=n_estimators)\n', 'bagging.fit(X, y)\n', '\n', '# Predict\n', 'X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n', 'y_tree = tree.predict(X_test)\n', 'y_tree_ = tree_.predict(X_test)\n', 'y_bagging = bagging.predict(X_test)\n', '# Plot the results\n', 'plt.figure(figsize=(12, 8))\n', 'plt.plot(X, y, \'o\', c=""g"", label=""data"")\n', 'plt.plot(X_test,y_tree,\'o\', c=""b"", label = ""tree(depth=1)"")\n', 'plt.plot(X_test,y_tree_,\'o\', c=""y"", label = ""tree(depth=10)"")\n', 'plt.plot(X_test,y_bagging,\'o\', c=""r"", label = ""bagging"")\n', '# TODO add plots for Bagging/Tree\n', 'plt.title(""Decision Tree Regression"")\n', 'plt.legend(loc=1, numpoints=1)\n', 'plt.show()']",EXPLORE,MODEL
965,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1034447.ipynb,1034447,1,"['# Illustrate the role of L（可添加文字解释和每条曲线的标签）\n', 'plt.figure(figsize=(12, 8))\n', 'C= [2,4,6,8,10]\n', ""Colour = ['c','g','b','y','r']\n"", 'plt.plot(X, y, \'o\', c=""g"", label=""data"")\n', 'for i in range(0,5):\n', '    bagging = BaggingRegressor(n_estimators=C[i])\n', '    bagging.fit(X, y)\n', '    y_bagging = bagging.predict(X_test)\n', ""    plt.plot(X_test,y_bagging,'g-', c= Colour[i],label ='bagging')\n"", '    \n', '    \n', 'plt.show()']",EXPLORE,MODEL
966,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1034447.ipynb,1034447,2,"['# Illustrate the role of the tree depth(麻烦加一下文字解释和每条曲线的标签)\n', 'plt.figure(figsize=(12, 8))\n', 'C= [2,4,6,8,10]\n', ""Colour = ['c','g','b','y','r']\n"", 'plt.plot(X, y, \'o\', c=""g"", label=""data"")\n', 'for i in range(0,5):\n', '    tree_ = DecisionTreeRegressor(max_depth= C[i])\n', '    tree_.fit(X, y)\n', '    y_tree_ = tree_.predict(X_test)\n', ""    plt.plot(X_test,y_tree_,'g-', c= Colour[i],label ='bagging')\n"", '    \n', '    \n', 'plt.show()\n']",EXPLORE,MODEL
967,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1034447.ipynb,1034447,3,"['# biais ,variance(第三四五题不会，以下代码仅供参考）\n', '\n', '# Settings\n', 'n_repeat = 50       # Number of iterations for computing expectations\n', 'n_train = 50        # Size of the training set\n', 'n_test = 1000       # Size of the test set\n', 'noise = 0.1         # Standard deviation of the noise\n', 'np.random.seed(0)\n', '\n', '# Change this for exploring the bias-variance decomposition of other\n', '# estimators. This should work well for estimators with high variance (e.g.,\n', '# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n', '# linear models).\n', 'estimators = [(""Tree"", DecisionTreeRegressor()),\n', '              (""Bagging(Tree)"", BaggingRegressor(DecisionTreeRegressor()))]\n', '\n', 'n_estimators = len(estimators)\n', '\n', '# Generate data\n', 'def f(x):\n', '    x = x.ravel()\n', '\n', '    return np.sin(x)\n', '\n', '\n', '\n', 'for i in range(n_repeat):\n', '    X, y = generate(n_samples=n_train, noise=noise)\n', '    X_train.append(X)\n', '    y_train.append(y)\n', '\n', 'X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n', '\n', '# Loop over estimators to compare\n', 'for n, (name, estimator) in enumerate(estimators):\n', '    # Compute predictions\n', '    y_predict = np.zeros((n_test, n_repeat))\n', '\n', '    for i in range(n_repeat):\n', '        estimator.fit(X_train[i], y_train[i])\n', '        y_predict[:, i] = estimator.predict(X_test)\n', '\n', '    # Bias^2 + Variance + Noise decomposition of the mean squared error\n', '    y_error = np.zeros(n_test)\n', '\n', '    for i in range(n_repeat):\n', '        for j in range(n_repeat):\n', '            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n', '\n', '    y_error /= (n_repeat * n_repeat)\n', '\n', '    y_noise = np.var(y_test, axis=1)\n', '    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n', '    y_var = np.var(y_predict, axis=1)\n', '\n', '    print(""{0}: {1:.4f} (error) = {2:.4f} (bias^2) ""\n', '          "" + {3:.4f} (var) + {4:.4f} (noise)"".format(name,\n', '                                                      np.mean(y_error),\n', '                                                      np.mean(y_bias),\n', '                                                      np.mean(y_var),\n', '                                                      np.mean(y_noise)))\n', '    plt.figure(figsize=(12, 8))\n', '\n', '    # Plot figures\n', '    plt.subplot(2, n_estimators, n + 1)\n', '    plt.plot(X_test, f(X_test), ""b"", label=""$f(x)$"")\n', '    plt.plot(X_train[0], y_train[0], "".b"", label=""LS ~ $y = f(x)+noise$"")\n', '\n', '    for i in range(n_repeat):\n', '        if i == 0:\n', '            plt.plot(X_test, y_predict[:, i], ""r"", label=""$\\^y(x)$"")\n', '        else:\n', '            plt.plot(X_test, y_predict[:, i], ""r"", alpha=0.05)\n', '\n', '    plt.plot(X_test, np.mean(y_predict, axis=1), ""c"",\n', '             label=""$\\mathbb{E}_{LS} \\^y(x)$"")\n', '\n', '    plt.xlim([-5, 5])\n', '    plt.title(name)\n', '\n', '    if n == 0:\n', '        plt.legend(loc=""upper left"", prop={""size"": 11})\n', '\n', '    plt.subplot(2, n_estimators, n_estimators + n + 1)\n', '    plt.plot(X_test, y_error, ""r"", label=""$error(x)$"")\n', '    plt.plot(X_test, y_bias, ""b"", label=""$bias^2(x)$""),\n', '    plt.plot(X_test, y_var, ""g"", label=""$variance(x)$""),\n', '    plt.plot(X_test, y_noise, ""c"", label=""$noise(x)$"")\n', '\n', '    plt.xlim([-5, 5])\n', '    plt.ylim([0, 0.1])\n', '\n', '    if n == 0:\n', '        plt.legend(loc=""upper left"", prop={""size"": 11})\n', '\n', 'plt.show()\n']",EXPLORE,MODEL
1719,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1043186.ipynb,1043186,3,"['# Import libraries necessary for this project\n', 'import numpy as np\n', 'import pandas as pd\n', 'import renders as rs\n', 'from IPython.display import display # Allows the use of display() for DataFrames\n', '\n', '# Show matplotlib plots inline (nicely formatted in the notebook)\n', '%matplotlib inline\n', '\n', '# Load the wholesale customers dataset\n', 'try:\n', '    data = pd.read_csv(""customers.csv"")\n', ""    data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n"", '    print ""Wholesale customers dataset has {} samples with {} features each."".format(*data.shape)\n', 'except:\n', '    print ""Dataset could not be loaded. Is the dataset missing?""']",IMPORT,WRANGLE
1734,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1043186.ipynb,1043186,49,"['# TODO: Inverse transform the centers\n', 'log_centers = pca.inverse_transform(centers)\n', '\n', '# TODO: Exponentiate the centers\n', 'true_centers = np.exp(log_centers)\n', '\n', '\n', '# Display the true centers\n', ""segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n"", 'true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\n', 'true_centers.index = segments\n', 'display(true_centers)\n', '\n', 'print ""Data broken into quintiles:""\n', 'display(data.quantile([0.25,0.5,0.75,1.0]).round())']",EXPLORE,EVALUATE
1735,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1043186.ipynb,1043186,53,"['# Display the predictions\n', 'for i, pred in enumerate(sample_preds):\n', '    print ""Sample point"", i, ""predicted to be in Cluster"", pred\n', '    \n', 'print ""\\n\\nChosen samples of wholesale customers dataset:""\n', 'display(samples)\n', '\n', '\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', '\n', ""# check if sample's spending is closer to segment 0 or 1\n"", 'samples = samples.reset_index(drop = True)\n', 'df_diff = (np.abs(samples - true_centers.iloc[0]) < np.abs(samples - true_centers.iloc[1])).applymap(lambda x: 0 if x else 1)\n', '\n', '#add predictions to df\n', ""df_final = pd.concat([df_diff, pd.Series(sample_preds, name = 'PREDICTION')], axis = 1)\n"", '\n', '#visualise results\n', 'fig, ax = plt.subplots(1,1, figsize = (10,6))\n', '\n', 'print ""Cluster prediction for each sample for each feature:""\n', '\n', ""sns.heatmap(df_final, annot = True, cbar = False, square = True, yticklabels = ['client_{0}'.format(i) for i in indices])\n"", 'plt.xticks(rotation = 90)\n', 'plt.yticks(rotation = 0);']",EXPLORE,EVALUATE
1736,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1043186.ipynb,1043186,61,"[""# Display the clustering results based on 'Channel' data\n"", 'rs.channel_results(reduced_data, outliers, pca_samples)']",EXPLORE,EVALUATE
646,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1051445.ipynb,1051445,2,"['#sort data frame by amount field, this is useful because multiple years are present\n', ""bank_df.sort_values(by='amount', inplace=True)\n"", ""inc_df.sort_values(by='income', inplace=True)\n"", 'inc_df.head()']",WRANGLE,EXPLORE
649,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1051445.ipynb,1051445,5,"['#Cycle over different income bins and plot loan amount distribution for each\n', '#focus on finding the best bins\n', '#repeat for different SQL selection filters (junior liens, non-owner-occupied, VA/FHA/RHS loans, manufactured housing)\n', 'bin_min = 0\n', 'bin_max = 20\n', 'bin_step = 30\n', 'bin_stop = 561\n', 'pct_of_loans = OrderedDict({})\n', 'end_flag = False\n', 'first = True\n', 'while bin_max < bank_df.income.max():\n', '    bin_stats = OrderedDict({})\n', '    if end_flag == True:\n', '        break\n', '    if bin_max >= bin_stop:\n', '        end_flag = True\n', '        graph_bin = bank_df[(bank_df.income >= bin_min)] #create tail bin for income over bin_stop\n', '        print(""bin stop"", ""*""*50)\n', ""        bin_stats['name'] = str(bin_stop) + ' and up' #set name for tail bin\n"", '    else:\n', '        graph_bin = bank_df[(bank_df.income >=bin_min) & (bank_df.income < bin_max)] #create bin of of amount for selected income\n', ""        bin_stats['name'] = str(bin_min) + ' to ' + str(bin_max)\n"", '    #set descriptive stats\n', '\n', ""    bin_stats['std']= graph_bin.amount.std()\n"", ""    bin_stats['mean'] = graph_bin.amount.mean()\n"", ""    bin_stats['median'] = graph_bin.amount.median()\n"", ""    bin_stats['max_mult'] = graph_bin.mult.max()\n"", ""    bin_stats['med_mult'] = graph_bin.mult.median()\n"", ""    bin_stats['min_mult'] = graph_bin.mult.min()\n"", ""    bin_stats['q98'] = graph_bin.amount.quantile(q=.98)\n"", ""    bin_stats['q02'] = graph_bin.amount.quantile(q=.02)\n"", ""    bin_stats['q75'] = graph_bin.amount.quantile(q=.75)\n"", ""    bin_stats['q25'] = graph_bin.amount.quantile(q=.25)\n"", ""    bin_stats['iqr'] = bin_stats['q75'] - bin_stats['q25']\n"", ""    bin_stats['min_q'] = bin_stats['q25'] - 1.5 * bin_stats['iqr']\n"", ""    bin_stats['max_q'] = bin_stats['q75'] + 1.5 * bin_stats['iqr']\n"", ""    bin_stats['count of loans'] = graph_bin.amount.count()\n"", ""    bin_stats['pct of total loans'] = (graph_bin.amount.count() / bank_df.income.count()) * 100\n"", '\n', '    #iterate over dataframe bin to remove outliers using IQR * 1.5\n', '    count=1\n', '    while True:\n', '        #remove outliers\n', ""        start_max_q = bin_stats['max_q']\n"", '        if bin_max >= bin_stop and count == 1:\n', '            graph_bin = bank_df[(bank_df.income >= bin_min)] #create tail bin for income over bin_stop\n', '        else:\n', ""            graph_bin = graph_bin[(graph_bin.amount > bin_stats['min_q']) & (graph_bin.amount < bin_stats['max_q'])]\n"", ""        print('\\n\\niteration {num}'.format(num=count))\n"", '        #reset bin stats to reflect IQR cutoffs\n', ""        bin_stats['std']= graph_bin.amount.std()\n"", ""        bin_stats['mean'] = graph_bin.amount.mean()\n"", ""        bin_stats['median'] = graph_bin.amount.median()\n"", ""        bin_stats['max_mult'] = graph_bin.mult.max()\n"", ""        bin_stats['med_mult'] = graph_bin.mult.median()\n"", ""        bin_stats['min_mult'] = graph_bin.mult.min()\n"", ""        bin_stats['q98'] = graph_bin.amount.quantile(q=.98)\n"", ""        bin_stats['q02'] = graph_bin.amount.quantile(q=.02)\n"", ""        bin_stats['q75'] = graph_bin.amount.quantile(q=.75)\n"", ""        bin_stats['q25'] = graph_bin.amount.quantile(q=.25)\n"", ""        bin_stats['iqr'] = bin_stats['q75'] - bin_stats['q25']\n"", ""        bin_stats['min_q'] = bin_stats['q25'] - 1.5 * bin_stats['iqr']\n"", ""        bin_stats['max_q'] = bin_stats['q75'] + 1.5 * bin_stats['iqr']\n"", '\n', '        count+=1\n', '        for key, value in bin_stats.items(): #print descriptive stats\n', '            print(key,value)\n', '        \n', ""        if start_max_q == bin_stats['max_q'] or count > 10:\n"", '            break\n', '            \n', '    if first: \n', '        first = False\n', '        bin_df = pd.DataFrame(bin_stats, index=range(1)) #instantiate initial dataframe\n', '    else:\n', '        concat_df = pd.DataFrame(bin_stats, index=range(1)) #instantiate subsequent dataframe \n', '        bin_df = pd.concat([bin_df, concat_df], axis=0) #concatenate dataframes each cycle\n', '    \n', '    amt = graph_bin.amount#.apply(sqrt)\n', '    inc = graph_bin.income#.apply(sqrt) #square roots show much nicer distributions of amount to income\n', '    xmin = 0 #set minimum x axis value\n', '    xmax = 1000 #set maximum x axis value\n', '    xsteps= 1000 #set steps on x axis (bins)\n', '    bins = np.linspace(xmin,xmax, xsteps)\n', '    #print(bins)\n', '    bin_viz = plt.figure(figsize=(10,5))\n', ""    plt.hist(inc, bins, label='income', alpha=1)\n"", ""    plt.hist(amt, bins, label='amount', alpha=.5)\n"", ""    plt.title('income > '+str(bin_min)+' and < '+str(bin_max))\n"", ""    plt.legend(loc='upper right')\n"", '    plt.xticks(np.arange(xmin,xmax, 50))\n', '    ax = plt.subplot()\n', '    #ax.set_ylim(0, 2000)\n', '    plt.show(bin_viz)\n', '    bin_min=bin_max\n', '    bin_max += bin_step\n', ""    pct_of_loans[bin_max] = bin_stats['pct of total loans']\n"", '    \n', ""path = '../linked_dist_stats/'\n"", 'if not os.path.exists(path):\n', '    os.makedirs(path)\n', ""bin_df.to_csv(path+'IQR_{loanType}.csv'.format(loanType=loanType))""]",WRANGLE,EXPLORE
650,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1051445.ipynb,1051445,6,"['\n', 'bin_df.reset_index(inplace=True, drop=True) #set an integer index instead of index passed at creation\n', 'bin_df.head(20)']",WRANGLE,EXPLORE
334,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1058499.ipynb,1058499,27,"['##Before any thing, lets split the entire data set into training and test. \n', '# Split the data into a 70/30 train/test split\n', 'X_train, X_test, y_train, y_test = train_test_split(df.ingredients, df.cuisine_group, test_size=0.3)\n', 'unique_cuisine_group = df.cuisine_group.unique().tolist()\n', '##For us to be able to use any of the prediction models we need turn the cuisine groups from earlier into numerical labels.\n', '## We use the standard Label Encoder to this.\n', 'from sklearn.preprocessing import LabelEncoder\n', 'le= LabelEncoder()\n', 'le.fit(unique_cuisine_group)\n', 'y_train =  le.transform(y_train)\n', 'y_test =  le.transform(y_test)\n', 'le.classes_']",WRANGLE,MODEL
335,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1058499.ipynb,1058499,28,"['##Build the Document Term Matrix based on the vectorizer that was created earler. \n', 'train_simple_dtm = count_vect.transform(X_train)\n', 'test_simple_dtm = count_vect.transform(X_test)\n', '\n', '##Given that we have a set of feature vectors that have a binary occurence probability in a certain dish/document, we \n', '#look at the naive bayseien model as our initial model. \n', '# Create the model\n', 'bnb = BernoulliNB()\n', '# Fit the model to the training data\n', 'bnb.fit(train_simple_dtm, y_train)\n', '# Score the model against the test data\n', 'bnb.score(test_simple_dtm, y_test)']",WRANGLE,MODEL
481,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1059730.ipynb,1059730,23,"['print(""A Calendar Is A Markov Transition Matrix"")\n', 'table = [["" "", ""S"", ""M"", ""T"", ""W"", ""T"", ""F"", ""S""],\n', '         [""S"", ""0"", ""1"", ""0"", ""0"", ""0"", ""0"", ""0"",],\n', '         [""M"", ""0"", ""0"", ""1"", ""0"", ""0"", ""0"", ""0"",],\n', '         [""T"", ""0"", ""0"", ""0"", ""1"", ""0"", ""0"", ""0"",],\n', '         [""W"", ""0"", ""0"", ""0"", ""0"", ""1"", ""0"", ""0"",],\n', '         [""T"", ""0"", ""0"", ""0"", ""0"", ""0"", ""1"", ""0"",],\n', '         [""F"", ""0"", ""0"", ""0"", ""0"", ""0"", ""0"", ""1"",],\n', '         [""S"", ""1"", ""0"", ""0"", ""0"", ""0"", ""0"", ""0"",],]\n', 'print(tabulate(table, tablefmt=""fancy_grid"", numalign = ""center""))\n', 'print()']",WRANGLE,EXPLORE
482,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1059730.ipynb,1059730,24,"['print(""Day of Week Vector: Today is Friday"")\n', 'table = [[""S"", ""M"", ""T"", ""W"", ""T"", ""F"", ""S""],\n', '         [""0"", ""0"", ""0"", ""0"", ""0"", ""1"", ""0"",]]\n', 'print(tabulate(table, tablefmt=""fancy_grid"", numalign = ""center""))\n', 'print()']",WRANGLE,EXPLORE
707,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1062193.ipynb,1062193,7,"[""df_housing_earnnet.sort_values(by='Working time req to rent 3-room',inplace=True)\n"", '# sorting before plotting\n', 'df_housing_earnnet.head()']",EXPLORE,WRANGLE
1650,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068026.ipynb,1068026,7,"['# Check the number of different displays\n', 'imageType = data.MediaName.unique()\n', 'len(imageType)']",EXPLORE,WRANGLE
1654,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068026.ipynb,1068026,11,"['# A class to check if the eye position falls into AOI \n', 'class AOI:\n', '    def init(self,aoitype,position,size):\n', ""    # AOItype = 'rectangle', 'circle'\n"", '    # pos = (x,y) position\n', '    # size = [width height] list \n', '    \n', '    # Check the format of AOI type and eye position \n', ""        if aoitype not in ['rectangle','circle']:\n"", '            raise Exception(""Error: aoitype is not recognized."")\n', '        else:\n', '            self.aoitype = aoitype\n', '    \n', '        if type(position) not in [tuple, list]:\n', '            raise Exception(""Error: AOI position should be an tuple or list"")\n', '        else:\n', '            self.position = position\n', '    \n', '    # check the AOI size\n', '        if type(size) == tuple:\n', '            size = [size[0],size[1]]\n', '        elif type(size) == list:\n', '            pass\n', '        else:\n', '            raise Exception(""Error: AOI size should be a list of integer values"")    \n', '        self.size = [int(size[0]),int(size[1])]\n', '\n', '    # Check if the eye position falls into AOI     \n', '    def contains(self, position):\n', '        inside = 0; \n', ""        if self.aoitype == 'circle':\n"", '            if (self.position[0]-position[0])**2 + (self.position[1]-position[1])**2 < self.r**2:\n', '                inside = 1;\n', ""        elif self.aoitype == 'rectangle':\n"", '            if (position[0] > self.position[0] and position[0] < self.position[0]+self.size[0]) and (position[1] > self.position[1] and position[1] < self.position[1]+self.size[1]):\n', '                inside = 1;\n', '        else:\n', '            raise Exception(""Error: aoitype is not recognized."")   \n', '        return inside ']",WRANGLE,MODEL
1657,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068026.ipynb,1068026,14,"['# Check if the eye positions fall into the left AOI \n', 'sub = AOI()\n', 'insideL = []\n', 'sub.init(AOItype,AOILposition,AOIsize)\n', 'for j in range (57, len(Position[0])+57):\n', '    xCoord = Position[0][j]\n', '    yCoord = Position[1][j]\n', '    p = [xCoord, yCoord]\n', '    if sub.contains(p) == 1:\n', '        insideL.append(p)']",WRANGLE,MODEL
1658,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068026.ipynb,1068026,15,"['# Check if the eye positions fall into the right AOI \n', 'insideR = []\n', 'sub.init(AOItype,AOIRposition,AOIsize)\n', 'for j in range (57, len(Position[1])+57):\n', '    xCoord = Position[0][j]\n', '    yCoord = Position[1][j]\n', '    p = [xCoord, yCoord]\n', '    # Store the inside range eye positions into a list \n', '    if sub.contains(p) == 1:\n', '        insideR.append(p)']",WRANGLE,EVALUATE
1107,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,8,"[""pop[('California', 2010):('Texas', 2000)]""]",WRANGLE,EXPLORE
1108,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,10,['pop[[i for i in pop.index if i[1] == 2010]]'],WRANGLE,EXPLORE
1109,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,13,"['index = pd.MultiIndex.from_tuples(index)\n', 'index']",WRANGLE,EXPLORE
1110,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,15,"['pop = pop.reindex(index)\n', 'pop']",WRANGLE,EXPLORE
1112,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,21,"['pop_df = pop.unstack()\n', 'pop_df']",WRANGLE,EXPLORE
1118,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,35,"[""pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])""]",EXPLORE,WRANGLE
1119,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,37,"[""pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])""]",EXPLORE,WRANGLE
1120,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,39,"[""pd.MultiIndex.from_product([['a', 'b'], [1, 2]])""]",EXPLORE,WRANGLE
1121,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,41,"[""pd.MultiIndex(levels=[['a', 'b'], [1, 2]],\n"", '              labels=[[0, 0, 1, 1], [0, 1, 0, 1]])']",EXPLORE,WRANGLE
1137,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,77,"['idx = pd.IndexSlice\n', ""health_data.loc[idx[:, 1], idx[:, 'HR']]""]",WRANGLE,EXPLORE
1140,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,85,"['data = data.sort_index()\n', 'data']",WRANGLE,EXPLORE
1145,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,94,"[""pop_flat = pop.reset_index(name='population')\n"", 'pop_flat']",EXPLORE,WRANGLE
1148,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1068488.ipynb,1068488,101,"[""data_mean = health_data.mean(level='year')\n"", 'data_mean']",WRANGLE,EXPLORE
643,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1071938.ipynb,1071938,31,"[""importance=pd.DataFrame({'words': vectorizer.get_feature_names(), 'importance': model.feature_importances_})\n"", ""importance[importance.importance>0].sort_values(by = 'importance').tail(20)""]",WRANGLE,EVALUATE
349,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1073105.ipynb,1073105,2,"['import numpy as np\n', 'from scipy import linalg as la\n', 'from os import walk\n', 'from scipy.ndimage import imread\n', 'from matplotlib import pyplot as plt\n', 'import matplotlib.cm as cm\n', 'import random\n', '\n', 'class FacialRec:\n', '     ##########Members##########\n', '     #   F, mu, Fbar, and U\n', '     ###########################\n', '     def __init__(self,path):\n', '         self.initFaces(path)\n', '         self.initMeanImage()\n', '         self.initDifferences()\n', '         self.initEigenfaces()\n', '     def initFaces(self, path):\n', '         self.F = getFaces(path)\n', '     def initMeanImage(self):\n', '         m,n = self.F.shape\n', '         self.mu = np.sum(self.F, axis = 1)/float(n)\n', '     def initDifferences(self):\n', '         m,n = self.F.shape\n', '         self.mu = self.mu.reshape(len(self.mu),1)\n', '         diff = np.tile(self.mu, n)\n', '         self.Fbar = self.F-diff\n', '     def initEigenfaces(self):\n', '         self.U, s, Vt = la.svd(self.Fbar, full_matrices =False)\n', '     def project(self, A, s=38):\n', '         A_s = np.dot(self.U.T, A)\n', '         return A_s\n', '     def findNearest(self, image, s=38):\n', '         image = image.reshape(len(image),1)\n', '         ghat = self.project(image - self.mu)\n', '         fhat = self.project(self.Fbar)\n', '         diff = (fhat.T-ghat.T).T\n', '         i = np.linalg.norm(diff, ord=2, axis =0)\n', '         return np.argmin(i)\n']",IMPORT,MODEL
355,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1073105.ipynb,1073105,14,"['def test1():\n', '    test_images = sampleFaces(7)\n', '    facial = FacialRec(""faces94"")\n', '    for i in test_images.T:\n', '        a = facial.findNearest(i)\n', '        show2(i, facial.F[:,a])\n', 'test1()']",EXPLORE,EVALUATE
1192,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1088637.ipynb,1088637,1,"['import numpy as np\n', 'def fitness(member):\n', '    member = np.asarray(member)\n', '    if member.ndim == 2:\n', '        _, n_genes = member.shape\n', '        indiv_fitnesses = member.sum(axis=1)\n', '        return indiv_fitnesses.mean()\n', '        \n', '    return member.sum()']",WRANGLE,MODEL
1193,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1088637.ipynb,1088637,3,"['def produce_offspring(parents, mutate=False, p=0.00):\n', '    n_parents, n_genes = parents.shape\n', '    gene_to_pull = np.random.randint(n_parents, size=n_genes)\n', '    child = [parents[gene_to_pull[k]][k] for k in range(n_genes)]\n', '    child = np.array(child)\n', '    \n', '    if mutate:\n', '        genes_to_flip = np.random.choice([0, 1], size=child.shape, p=[1-p, p])\n', '        i = np.argwhere(genes_to_flip == 1)\n', '        child[i] = 1 - child[i]\n', '        \n', '    return child']",WRANGLE,MODEL
1194,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1088637.ipynb,1088637,5,"['def one_generation(members, n_parents=2, **kwargs):\n', '    """""" members: 2D np.ndarray\n', '            members.shape = (n_parents, n_genes) """"""\n', '    parents = np.random.permutation(members) \n', '    children = []\n', '    for parent_group in range(len(parents) // n_parents):\n', '        parents_group = parents[parent_group * n_parents : (parent_group + 1) * n_parents]\n', '        children += [produce_offspring(parents_group, **kwargs) for _ in range(2*n_parents)]\n', '    children = np.array(children)\n', '    # make sure we produce (approximately) 2*N children when we have N parents\n', '    assert np.abs(len(children) - 2*len(parents)) < n_mates + 2\n', '    return children ']",WRANGLE,MODEL
1195,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1088637.ipynb,1088637,7,"['def evolve(n_parents=2000, n_mates=2, n_genes=200, n_generations=100, p_fit=0.5, verbose=10):\n', '    parents = np.random.choice([0, 1], size=(n_parents, n_genes), p=[1-p_fit, p_fit])\n', '    child = produce_offspring(parents)\n', '\n', '    data = []\n', '    for generation in range(n_generations):\n', '        if verbose and generation % verbose == 0:\n', ""            print('Generation {} for n_mates = {} with {} parents'.format(generation, n_mates, len(parents)))\n"", '            \n', '        children = one_generation(parents, n_parents=n_mates, mutate=True, p=0.01)\n', '        \n', '        # kill half the children\n', '        children_fitness = np.array([fitness(child) for child in children])\n', '        i = np.argsort(children_fitness)\n', '        children = children[i]\n', '        parents = children[len(children) // 2:].copy()\n', '        \n', ""        data += [{'fitness': fitness(parents), 'generation': generation,\n"", ""                  'n_mates': n_mates, 'n_parents': n_parents,\n"", ""                  'n_genes': n_genes, 'n_generations': n_generations}]\n"", '        \n', '    return data']",WRANGLE,MODEL
1197,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1088637.ipynb,1088637,11,"['from altair import Chart\n', 'import pandas as pd\n', '\n', 'df = pd.DataFrame(data)\n', 'Chart(df).mark_line().encode(\n', ""    x='generation', y='fitness', color='n_mates')\n"", '    ']",EXPLORE,EVALUATE
373,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1089165.ipynb,1089165,17,"['def plot_decision_regions(X,y,classifier,resolution=0.02):\n', ""    markers = ('s', 'x', 'o', '^', 'v')\n"", ""    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n"", '    cmap = ListedColormap(colors[:len(np.unique(y))])\n', '    x1_min, x1_max = X[:,0].min() -1, X[:,0].max() + 1\n', '    x2_min, x2_max = X[:,1].min() -1, X[:,1].max() + 1\n', '    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max,resolution),np.arange(x2_min, x2_max, resolution))\n', '    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n', '    Z = Z.reshape(xx1.shape)\n', '    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n', '    plt.xlim(xx1.min(), xx1.max())\n', '    plt.ylim(xx2.min(), xx2.max())\n', '    for idx, cl in enumerate(np.unique(y)):\n', '        plt.scatter(x=X[y==cl, 0], y=X[y==cl,1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl)\n', '        ']",EXPLORE,EVALUATE
374,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1089165.ipynb,1089165,18,"['plot_decision_regions(X,y,classifier=ppn)\n']",EXPLORE,EVALUATE
380,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1089165.ipynb,1089165,24,['plt.show()'],EXPLORE,EVALUATE
570,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1089313.ipynb,1089313,0,"['from sklearn import datasets\n', 'from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n', '\n', 'digits = datasets.load_digits()\n', 'cancer = datasets.load_breast_cancer()']",IMPORT,WRANGLE
1467,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1090152.ipynb,1090152,4,"[""event_counts = data['Event'].value_counts()\n"", 'print len(event_counts)']",EXPLORE,WRANGLE
1469,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1090152.ipynb,1090152,6,"[""sport_counts = data['Sport'].value_counts()\n"", 'print len(sport_counts)']",EXPLORE,WRANGLE
1474,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1090152.ipynb,1090152,11,"['def isint(x):\n', '    try:\n', '        a = float(x)\n', '        b = int(a)\n', '    except ValueError:\n', '        return False\n', '    else:\n', '        return a == b']",WRANGLE,EXPLORE
1291,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1090484.ipynb,1090484,4,"[""pastel = sb.color_palette('pastel')\n"", ""sb.set_context('talk')""]",WRANGLE,EXPLORE
504,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1096612.ipynb,1096612,5,"[""FORMULA1 = ('treatment ~ inwyr07_f + yrbrn60_f + yrbrn60_f2 + '\n"", ""            'edurank_f + hincrank_f +'\n"", ""            'tvtot_f + rdtot_f + nwsptot_f')\n"", '\n', 'def compute_delta(group, country):\n', ""    group['yrbrn60_f2'] = group.yrbrn60_f ** 2\n"", ""    group['propensity'] = np.nan\n"", ""    group['treatment'] = np.nan\n"", '        \n', '    # quantize netuse to get treatment variable\n', '    # choose threshold close to the median\n', '    netuse = group.netuse_f\n', '    thresh = netuse.median()\n', '    if thresh < 1:\n', '        thresh = 1\n', '    group.treatment = (netuse >= thresh).astype(int)\n', '\n', '    # compute propensities\n', '    model = smf.logit(FORMULA1, data=group)    \n', '    results = model.fit(disp=False)\n', '    group.propensity = results.predict(group)\n', '    \n', '    # divide into treatment and control groups\n', '    treatment = group[group.treatment == 1]\n', '    control = group[group.treatment == 0]\n', '    \n', '    # sort the propensities of the controls (for fast lookup)\n', '    series = control.propensity.sort_values()\n', '\n', '    # look up the propensities of the treatment group\n', '    # to find (approx) closest matches in the control group\n', '    indices = series.searchsorted(treatment.propensity)\n', '    indices[indices < 0] = 0\n', '    indices[indices >= len(control)] = len(control)-1\n', '    \n', '    # use the indices to select the matches\n', '    control_indices = series.index[indices]\n', '    matches = control.loc[control_indices]\n', '\n', '    # find distances and differences\n', '    distances = (treatment.propensity.values - \n', '                 matches.propensity.values)\n', '    differences = (treatment.rlgdgr_f.values - \n', '                   matches.rlgdgr_f.values)\n', '    \n', '    # select differences with small distances\n', '    caliper = differences[abs(distances) < 0.001]\n', '\n', '    # return the mean difference\n', '    delta = np.mean(caliper)\n', '    return delta']",WRANGLE,MODEL
505,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1096612.ipynb,1096612,6,"['def process_frame(df, country_map):\n', ""    grouped = df.groupby('cntry')\n"", '    for code, group in grouped:\n', '        country = country_map[code]\n', '\n', '        # compute mean difference between matched pairs\n', '        delta = compute_delta(group, country)\n', '        d = dict(delta=delta)\n', '        country.add_params(d)']",WRANGLE,MODEL
506,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1096612.ipynb,1096612,7,"['def process_all_frames(store, country_map, num=201):\n', '    """"""Loops through the store and processes frames.\n', '    \n', '    store: store\n', '    country_map: map from code to Country\n', '    num: how many resamplings to process\n', '    reg_func: function used to compute regression\n', '    formula: string Patsy formula\n', ""    model_num: which model we're running\n"", '    """"""\n', '    for i, key in enumerate(store.keys()):\n', '        if i >= num:\n', '            break\n', '        print(i, key)\n', '        df = store.get(key)\n', '        process_frame(df, country_map)']",WRANGLE,MODEL
507,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1096612.ipynb,1096612,8,"['process_all_frames(store, country_map, num=101)']",WRANGLE,MODEL
513,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1096612.ipynb,1096612,15,"[""xlabel1 = 'Difference in religiosity (10 point scale)'""]",WRANGLE,EXPLORE
514,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1096612.ipynb,1096612,16,"['xlim = [-2.5, 1.0]']",WRANGLE,EXPLORE
1341,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1098281.ipynb,1098281,1,"[""engine = create_engine('postgresql://{}:{}@{}/{}'.format(\n"", ""                os.environ['PGUSER'], os.environ['PGPASSWORD'],\n"", ""                os.environ['PGHOST'], os.environ['PGDATABASE']))""]",WRANGLE,IMPORT
1342,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1098281.ipynb,1098281,2,"['import features\n', 'db = features.FeatureStorage()']",WRANGLE,IMPORT
1213,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1103327.ipynb,1103327,7,"[""recent = df[df['year'] == 2014]\n"", 'recent.head(3)']",EXPLORE,WRANGLE
579,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1103777.ipynb,1103777,7,"['ros = wqio.robustros.RobustROSEstimator(data=df)\n', 'np.median(ros.estimated_values)']",EXPLORE,MODEL
580,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1103777.ipynb,1103777,9,"['nsqdata.season_medians.query(""parameter == \'Cadmium (Cd)\' and season == \'spring\'"")']",EXPLORE,EVALUATE
305,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1105414.ipynb,1105414,1,"['import sys\n', ""sys.path.append('D:\\Andres\\Documents\\Python Scripts')\n"", '\n', 'import conexiones\n', '\n', 'cStr = conexiones.cnxPg.format(""Cuyucha"")']",WRANGLE,IMPORT
1424,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,7,"['for message_no, message in enumerate(messages[:10]):\n', '    print(message_no, message)\n', ""    print('\\n')""]",WRANGLE,EXPLORE
1429,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,19,"[""messages['length'] = messages['message'].apply(len)\n"", 'messages.head()']",EXPLORE,WRANGLE
1436,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,34,"['from nltk.corpus import stopwords\n', ""stopwords.words('english')[0:10] # Show some stop words""]",EXPLORE,IMPORT
1445,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,51,"['# Might take awhile...\n', ""bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])\n"", '\n', '# Print total number of vocab words\n', 'print(len(bow_transformer.vocabulary_))']",EXPLORE,WRANGLE
1446,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,53,"[""message4 = messages['message'][3]\n"", 'print(message4)']",EXPLORE,WRANGLE
1447,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,55,"['bow4 = bow_transformer.transform([message4])\n', 'print(bow4)\n', 'print(bow4.shape)']",EXPLORE,WRANGLE
1448,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,57,"['print(bow_transformer.get_feature_names()[4073])\n', 'print(bow_transformer.get_feature_names()[9570])']",EXPLORE,WRANGLE
1454,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,67,"['messages_tfidf = tfidf_transformer.transform(messages_bow)\n', 'print(messages_tfidf.shape)']",WRANGLE,EXPLORE
1456,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,74,"[""print('predicted:', spam_detect_model.predict(tfidf4)[0])\n"", ""print('expected:', messages.label[3])""]",EXPLORE,EVALUATE
1457,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,76,"['all_predictions = spam_detect_model.predict(messages_tfidf)\n', 'print(all_predictions)']",MODEL,EVALUATE
1462,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108440.ipynb,1108440,87,['predictions = pipeline.predict(msg_test)'],MODEL,EVALUATE
453,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108807.ipynb,1108807,2,"['ImageIndex = 0\n', '\n', 'def PixelError(Window):\n', '    return np.sum(Window)\n', '\n', 'ResultImages = []\n', 'DepthMaps = []\n', 'for Input in Inputs:\n', '    Deblurred = []\n', '    for Index in range(9):\n', '        Deblurred.append(io.loadmat(""%s_%d"" % (Input, Index + 1))[\'deblurred\'])\n', '        \n', '    TargetImage = Images[ImageIndex]\n', '    \n', ""    ErrorsR = map(lambda i: (TargetImage[:, :, 0] - filters.convolve(Deblurred[i][:, :, 0], DepthFilters[i], mode='reflect')) ** 2, range(9))\n"", ""    ErrorsG = map(lambda i: (TargetImage[:, :, 1] - filters.convolve(Deblurred[i][:, :, 1], DepthFilters[i], mode='reflect')) ** 2, range(9))\n"", ""    ErrorsB = map(lambda i: (TargetImage[:, :, 2] - filters.convolve(Deblurred[i][:, :, 2], DepthFilters[i], mode='reflect')) ** 2, range(9))\n"", '    \n', ""    PxErrorsR = map(lambda i: filters.generic_filter(ErrorsR[i], PixelError, size=(3, 3), mode='reflect'), range(9))\n"", ""    PxErrorsG = map(lambda i: filters.generic_filter(ErrorsG[i], PixelError, size=(3, 3), mode='reflect'), range(9))\n"", ""    PxErrorsB = map(lambda i: filters.generic_filter(ErrorsB[i], PixelError, size=(3, 3), mode='reflect'), range(9))\n"", '    \n', '    BestR = np.argmin(PxErrorsR, axis=0)\n', '    BestG = np.argmin(PxErrorsG, axis=0)\n', '    BestB = np.argmin(PxErrorsB, axis=0)\n', '    \n', '    AllInFocusImage = np.zeros_like(TargetImage)\n', '    \n', '    for h in range(AllInFocusImage.shape[0]):\n', '        for w in range(AllInFocusImage.shape[1]):\n', '            AllInFocusImage[h, w, 0] = Deblurred[BestR[h, w]][h, w, 0]\n', '            AllInFocusImage[h, w, 1] = Deblurred[BestG[h, w]][h, w, 1]\n', '            AllInFocusImage[h, w, 2] = Deblurred[BestB[h, w]][h, w, 2]\n', '    \n', '    DepthMaps.append(BestR)\n', '    ResultImages.append(AllInFocusImage)\n', '\n', '    ImageIndex += 1\n', '    ']",WRANGLE,MODEL
454,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1108807.ipynb,1108807,3,"['for i in range(3):    \n', '    subplot(3, 2, (i * 2) + 1)\n', '    imshow(Images[i])\n', '    subplot(3, 2, (i * 2) + 2)\n', '    imshow(np.clip(ResultImages[i], 0, 1))\n', '    imageio.imsave(""%s_deblurred.bmp"" % Inputs[i], np.clip(ResultImages[i], 0, 1))']",EXPLORE,WRANGLE
1571,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,12,"['def score_one_patient2( detection_folder,\n', '                        ground_truth_folder,\n', '                        f,\n', '                        brain_ground_truth ):\n', '    \n', '    patient_id = os.path.basename(f)\n', '    ground_truth_file = ground_truth_folder + ""/"" + patient_id + ""_landmarks.nii.gz""\n', '    \n', '    if not os.path.exists(ground_truth_file):\n', '        print ""missing ground thruth"", ground_truth_file\n', '        return [np.inf, np.inf, np.inf, np.inf, np.inf,np.inf]\n', '    \n', '    labels = irtk.imread( ground_truth_file )\n', '    \n', '    # ground truth\n', ""    center_left_lung = labels.ImageToWorld( np.array(nd.center_of_mass( (labels == 1).view(np.ndarray) ),dtype='float32')[::-1])\n"", ""    center_right_lung = labels.ImageToWorld( np.array(nd.center_of_mass( (labels == 2).view(np.ndarray) ),dtype='float32')[::-1])\n"", ""    center_heart = labels.ImageToWorld( np.array(nd.center_of_mass( (labels == 3).view(np.ndarray) ),dtype='float32')[::-1])\n"", ""    center_liver = labels.ImageToWorld( np.array(nd.center_of_mass( (labels == 4).view(np.ndarray) ),dtype='float32')[::-1])\n"", ""    #center_brain = labels.ImageToWorld( np.array(nd.center_of_mass( (labels == 5).view(np.ndarray) ),dtype='float32')[::-1])\n"", '   \n', ""    liver_z = abs( np.array(nd.center_of_mass( (labels == 4).view(np.ndarray) ),dtype='float32')[0] -\n"", '                   labels.shape[0]/2 )\n', '    \n', '    if patient_id in brain_ground_truth:\n', '        center_brain = brain_ground_truth[patient_id]\n', '    else:\n', '        print patient_id, ""not in brainboxes.tsv""\n', ""        center_brain = labels.ImageToWorld( np.array(nd.center_of_mass( (labels == 5).view(np.ndarray) ),dtype='float32')[::-1])\n"", '        \n', '    # brain detection\n', '    #brain_mask = irtk.imread(f+""/brain_mask.nii.gz"")\n', '    brain_mask = f+""/brain_mask.nii.gz""\n', '    if not os.path.exists(brain_mask):\n', '        print ""brain not found"",patient_id\n', '        return [100, 100, 100, 100, 100, liver_z]\n', '    \n', ""    #detected_brain = brain_mask.ImageToWorld( np.array(nd.center_of_mass( (brain_mask == 1).view(np.ndarray) ),dtype='float32')[::-1])\n"", '    detected_brain = get_center_brain_detection(brain_mask)\n', '    \n', '    # second stage\n', '    landmarks = irtk.imread( f + ""/prediction_2/landmarks.nii.gz"" )\n', '    #landmarks = irtk.imread( f + ""/prediction_2/final_seg.nii.gz"" )\n', '    \n', '    if landmarks is None:\n', '        print ""something wrong for patient"",patient_id\n', '        return [100, 100, 100, 100, 100, liver_z]\n', '\n', ""    detected_left_lung = landmarks.ImageToWorld( np.array(nd.center_of_mass( (landmarks == 1).view(np.ndarray) ),dtype='float32')[::-1])\n"", '    \n', ""    detected_right_lung = landmarks.ImageToWorld( np.array(nd.center_of_mass( (landmarks == 2).view(np.ndarray) ),dtype='float32')[::-1])\n"", '\n', ""    detected_heart = landmarks.ImageToWorld( np.array(nd.center_of_mass( (landmarks == 3).view(np.ndarray) ),dtype='float32')[::-1])\n"", '    \n', ""    detected_liver = landmarks.ImageToWorld( np.array(nd.center_of_mass( (landmarks == 4).view(np.ndarray) ),dtype='float32')[::-1])\n"", '        \n', '    error_left_lung = np.linalg.norm( center_left_lung - detected_left_lung )\n', '    error_right_lung = np.linalg.norm( center_right_lung - detected_right_lung )\n', '    error_heart = np.linalg.norm( center_heart - detected_heart )\n', '    error_liver = np.linalg.norm( center_liver - detected_liver )\n', '    error_brain = np.linalg.norm( center_brain - detected_brain )\n', '\n', '    return [ error_left_lung, error_right_lung, error_heart, error_liver, error_brain, liver_z ]\n', '\n', 'def score_all2( detection_folder,\n', '               ground_truth_folder,\n', '               n_jobs=10 ):\n', '        \n', '    all_folders = sorted( glob( detection_folder+""/*"" ) )\n', '        \n', '    original_folder = ""/vol/vipdata/data/fetal_data/motion_correction""\n', '    brain_ground_truth = {}\n', '    reader = csv.reader( open( original_folder + ""/brainboxes.tsv"", ""rb""), delimiter=""\\t"" )\n', '    for patient_id, filename, sct, corner, size in reader:\n', '        file_id = filename[:-len("".nii"")]\n', ""        brain_ground_truth[file_id] = get_box_center(map(float,corner.split(',')),\n"", ""                                                      map(float,size.split(',')),\n"", '                                                      original_folder+""/original_scans/""+filename)\n', '       \n', '    all_folders_no_thick_slices = []\n', '    for f in all_folders:\n', '        file_id = os.path.basename(f)\n', ""        header = irtk.imread( original_folder + '/original_scans/'+file_id+'.nii',\n"", '                              empty=True, \n', '                              force_neurological=False ).get_header()\n', ""        if header['pixelSize'][2] > 2.0*header['pixelSize'][0]:\n"", '            print ""Thick slices:"", os.path.basename(f), header[\'pixelSize\']\n', '        else:\n', '            all_folders_no_thick_slices.append(f)\n', '            \n', '    all_folders = all_folders_no_thick_slices\n', '        \n', '    ga_file = ""/vol/vipdata/data/fetal_data/motion_correction/ga.tsv""\n', '    all_ga = {}\n', '    reader = csv.reader( open( ga_file, ""rb""), delimiter="" "" )\n', '    for patient_id, ga in reader:\n', '        all_ga[patient_id] = float(ga)\n', '        \n', '    scores = Parallel(n_jobs=n_jobs)(delayed(score_one_patient2)( detection_folder,\n', '                                                                  ground_truth_folder,\n', '                                                                  f,\n', '                                                                  brain_ground_truth )\n', '                                               for f in all_folders )\n', '        \n', '    patient_id, scores = map(os.path.basename, all_folders), np.array(scores)\n', ""    return pd.DataFrame({ 'patient_id' : patient_id,\n"", ""                          'Left lung' : scores[:,0],\n"", ""                          'Right lung' : scores[:,1],\n"", ""                          'Heart' : scores[:,2],\n"", ""                          'Liver' : scores[:,3],\n"", ""                          'Brain' : scores[:,4],\n"", ""                          'liver_z' : scores[:,5],\n"", ""                          'ga' : map( lambda x: all_ga[x.split('_')[0]], patient_id )})""]",WRANGLE,EVALUATE
1572,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,13,"['def count_inside_ellipsoid_one_patient_motion( f,\n', '                              ground_truth_folder,\n', '                              padding,\n', '                              scale,\n', '                              debug):\n', '    \n', '    patient_id = os.path.basename(f)\n', '    f_labels = ground_truth_folder + ""/"" + patient_id + ""_landmarks.nii.gz""\n', '    f_img = f + ""/prediction_2/img.nii.gz""\n', '    \n', '    f_shape_model = ""/vol/medic02/users/kpk09/OUTPUT/stage1_shape/2379/shape_model.pk""\n', ""    shape_model = pickle.load( open( f_shape_model, 'rb' ) )\n"", '    \n', '    if debug:\n', '        debug_ellipsoid_motion( f_labels, shape_model, scale )\n', '        \n', '    labels = irtk.imread(f_labels)\n', '    img = irtk.imread( f_img )\n', ""    labels = labels.transform( target=img, interpolation='nearest' )        \n"", '        \n', '    u,v,w = get_orientation_motion(labels)\n', ""    M = np.array( [w,v,u], dtype='float32' )\n"", '    \n', ""    mean_heart = np.array(nd.center_of_mass( (labels == 3).view(np.ndarray) ),dtype='float32')\n"", ""    mean_left_lung = np.array(nd.center_of_mass( (labels == 1).view(np.ndarray) ),dtype='float32')\n"", ""    mean_right_lung = np.array(nd.center_of_mass( (labels == 2).view(np.ndarray) ),dtype='float32')\n"", ""    mean_liver = np.array(nd.center_of_mass( (labels == 4).view(np.ndarray) ),dtype='float32')\n"", '    \n', ""    cov_heart = shape_model['cov_heart']*scale**2\n"", ""    cov_left_lung = shape_model['cov_left_lung']*scale**2\n"", ""    cov_right_lung = shape_model['cov_right_lung']*scale**2\n"", ""    cov_liver = shape_model['cov_liver']*scale**2\n"", '    \n', '    landmarks = irtk.imread( f + ""/prediction_2/landmarks.nii.gz"" )\n', '    \n', '    if landmarks is None:\n', '        print ""something wrong for patient"",patient_id\n', '        return [0,0,0,0,0]\n', '    \n', ""    detected_left_lung = np.array(nd.center_of_mass( (landmarks == 1).view(np.ndarray) ),dtype='float32') - padding\n"", ""    detected_right_lung = np.array(nd.center_of_mass( (landmarks == 2).view(np.ndarray) ),dtype='float32') - padding\n"", ""    detected_heart = np.array(nd.center_of_mass( (landmarks == 3).view(np.ndarray) ),dtype='float32') - padding\n"", ""    detected_liver = np.array(nd.center_of_mass( (landmarks == 4).view(np.ndarray) ),dtype='float32') - padding\n"", '      \n', '    inside_left_lung = is_in_ellipsoid_motion( detected_left_lung, mean_left_lung, cov_left_lung, (u,v,w), mean_heart )\n', '    inside_right_lung = is_in_ellipsoid_motion( detected_right_lung, mean_right_lung, cov_right_lung, (u,v,w), mean_heart )\n', '    inside_heart = is_in_ellipsoid_motion( detected_heart, mean_heart, cov_heart, (u,v,w), mean_heart )\n', '    inside_liver = is_in_ellipsoid_motion( detected_liver, mean_liver, cov_liver, (u,v,w), mean_heart )\n', '    \n', '    return [inside_left_lung, inside_right_lung, inside_heart, inside_liver,\n', '            inside_left_lung and inside_right_lung and inside_heart and inside_liver]\n', '\n', 'def debug_ellipsoid_motion( f_labels, f_img, shape_model, scale ):\n', '    labels = irtk.imread(f_labels)\n', '    img = irtk.imread(f_img)\n', ""    labels = labels.transform( target=img, interpolation='nearest' ) \n"", '    \n', '    u,v,w = get_orientation_motion(labels)\n', ""    M = np.array( [w,v,u], dtype='float32' )\n"", '    \n', ""    mean_heart = np.array(nd.center_of_mass( (labels == 3).view(np.ndarray) ),dtype='float32')\n"", ""    mean_left_lung = np.array(nd.center_of_mass( (labels == 1).view(np.ndarray) ),dtype='float32')\n"", ""    mean_right_lung = np.array(nd.center_of_mass( (labels == 2).view(np.ndarray) ),dtype='float32')\n"", ""    mean_liver = np.array(nd.center_of_mass( (labels == 4).view(np.ndarray) ),dtype='float32')\n"", '    \n', ""    cov_heart = shape_model['cov_heart']*scale**2\n"", ""    cov_left_lung = shape_model['cov_left_lung']*scale**2\n"", ""    cov_right_lung = shape_model['cov_right_lung']*scale**2\n"", ""    cov_liver = shape_model['cov_liver']*scale**2\n"", '    \n', ""    res = irtk.zeros( labels.get_header(), dtype='uint8' )\n"", '    points = np.argwhere( res == 0 )\n', '    for i, (mean, cov) in enumerate(zip( [mean_heart, mean_left_lung, mean_right_lung, mean_liver], \n', '                                       [cov_heart, cov_left_lung, cov_right_lung, cov_liver] ),\n', '                                  start=1):\n', '        inv_cov = np.linalg.pinv(cov)\n', '        pts = np.transpose( np.dot( M, np.transpose(points - mean_heart)))\n', '        mean = np.dot( M, mean - mean_heart)\n', '        selection = np.sum( np.dot(pts-mean, inv_cov) * (pts-mean), 1) < 1\n', '        pts = points[selection]\n', '        res[pts[:,0],\n', '            pts[:,1],\n', '            pts[:,2]] = i\n', '        \n', '    irtk.imwrite( ""tmp/debug_ellipsoids_motion/""+os.path.basename(f), res )\n', '    return \n', '\n', 'def is_in_ellipsoid_motion( p, mean, cov, (u,v,w), heart_center ):\n', ""    M = np.array( [w,v,u], dtype='float32' )\n"", '    p = np.dot( M, p - heart_center)\n', '    mean = np.dot( M, mean - heart_center)\n', '    inv_cov = np.linalg.pinv( cov )\n', '    return np.dot( np.transpose(p-mean),np.dot(inv_cov,p-mean) ) < 1\n', '\n', 'def count_inside_ellipsoid_motion( detection_folder,\n', '                      ground_truth_folder,\n', '                      n_jobs=10,\n', '                      padding=0,\n', '                      scale=1.0,\n', '                      debug=False):\n', '        \n', '    all_folders = sorted( glob( detection_folder+""/*"" ) )\n', '       \n', '    original_folder = ""/vol/vipdata/data/fetal_data/motion_correction""\n', '    all_folders_no_thick_slices = []\n', '    for f in all_folders:\n', '        file_id = os.path.basename(f)\n', ""        header = irtk.imread( original_folder + '/original_scans/'+file_id+'.nii',\n"", '                              empty=True, \n', '                              force_neurological=False ).get_header()\n', ""        if header['pixelSize'][2] > 2.0*header['pixelSize'][0]:\n"", '            print ""Thick slices:"", os.path.basename(f), header[\'pixelSize\']\n', '        else:\n', '            all_folders_no_thick_slices.append(f)\n', '            \n', '    all_folders = all_folders_no_thick_slices\n', '\n', '    scores = Parallel(n_jobs=n_jobs)(delayed(count_inside_ellipsoid_one_patient_motion)( f,\n', '                                                                 ground_truth_folder,\n', '                                                                 padding,\n', '                                                                 scale=scale,\n', '                                                                 debug=debug)\n', '                                               for f in all_folders )\n', '        \n', '    patient_id, scores = map(os.path.basename, all_folders), np.array(scores)\n', '    \n', ""    return pd.DataFrame( { 'patient_id' : patient_id,\n"", ""                           'Left lung' : scores[:,0],\n"", ""                           'Right lung' : scores[:,1],\n"", ""                           'Heart' : scores[:,2],\n"", ""                           'Liver' : scores[:,3],\n"", ""                           'All in' : scores[:,4]} ).convert_objects(convert_numeric=True)""]",WRANGLE,EVALUATE
1573,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,14,"['detection_folder = ""/vol/vipdata/data/fetal_data/OUTPUT/whole_body_shape_padding50""\n', 'ground_truth_folder = ""/vol/vipdata/data/fetal_data/motion_correction/landmarks""\n', 'data_motion = score_all2(detection_folder, ground_truth_folder,n_jobs=20 )\n', 'data_motion[[""Left lung"",""Right lung"",""Heart"",""Liver"",""Brain""]].hist();\n', '\n', 'plt.figure()\n', 'ax = data_motion[[""Left lung"",""Right lung"",""Heart"",""Liver"",""Brain""]].boxplot(return_type=\'axes\',\n', ""                                                                      flierprops={'marker':'.','color':[0,0,0]},\n"", '                                                                      fontsize=18);\n', 'ax.set_ylim([0,40])\n', 'ax.yaxis.set_ticks(np.arange(0, 41, 5))\n', 'ax.set_ylabel(""Error distance (mm)"")\n', ""ax.set_title('Error distance for second dataset');\n"", 'plt.savefig(""real_case_scenario.pdf"", bbox_inches=""tight"");\n', '\n', 'print ""how many times the detected heart is within 10mm of the true heart?""\n', ""print float(np.sum(data_motion['Heart'] <= 10))/len(data_motion.index)\n"", '\n', 'print ""how many times the error for the lungs is less than 15mm""\n', ""print float(np.sum(np.logical_and(data_motion['Left lung'] <= 15,\n"", ""                            data_motion['Right lung'] <= 15)))/len(data_motion.index)\n"", '\n', 'print ""how many times the error for the liver is less than 20mm""\n', ""print float(np.sum(data_motion['Liver'] <= 20))/len(data_motion.index)""]",EXPLORE,EVALUATE
1574,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,15,"['detection_folder = ""/vol/vipdata/data/fetal_data/OUTPUT/whole_body_shape_padding50""\n', 'ground_truth_folder = ""/vol/vipdata/data/fetal_data/motion_correction/landmarks""\n', 'data_motion_ellipsoid = count_inside_ellipsoid_motion(detection_folder,\n', '                                            ground_truth_folder,\n', '                                            n_jobs=-1, \n', '                                            padding=10, \n', '                                            debug=False )\n', '\n', ""plt.rc('text', usetex=False)\n"", 'data_motion_ellipsoid.hist();\n', 'print data_motion_ellipsoid.sum()\n', '\n', 'print ""Motion subjects""\n', 'for cl in [""Left lung"",""Right lung"",""Heart"",""Liver""]:\n', ""    binning = np.bincount(data_motion_ellipsoid[cl].astype('int32'),minlength=2)\n"", ""    print cl, binning, binning.astype('float') / len(data_motion_ellipsoid.index) * 100""]",EXPLORE,EVALUATE
1575,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,16,"['ga_file = ""/vol/vipdata/data/fetal_data/motion_correction/ga.tsv""\n', '\n', 'all_ga = {}\n', 'reader = csv.reader( open( ga_file, ""rb""), delimiter="" "" )\n', 'for patient_id, ga in reader:\n', '    all_ga[patient_id] = float(ga)\n', '    \n', 'def get_BPD( ga ):\n', '    """"""50th centile from Snijders""""""\n', '    p = np.poly1d( [ -3.20651640e-03,\n', '                     2.14380813e-01,\n', '                     -1.47907551e+00,\n', '                     1.87142471e+01 ] )\n', '    return p(ga)\n', '\n', 'all_bpd = []\n', ""for file_id in data_motion['patient_id']:\n"", ""    patient_id = file_id.split('_')[0]\n"", '    all_bpd.append( get_BPD( all_ga[patient_id] ) )\n', '    \n', 'all_bpd = np.array( all_bpd )\n', '\n', ""print data_motion[data_motion['Brain']==data_motion['Brain'].max()]\n"", ""print np.sum( data_motion['Brain'] < all_bpd/4 )""]",WRANGLE,EVALUATE
1577,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,18,"['import cPickle as pk\n', '\n', ""pk.dump(data_motion,open('data_motion.pk', 'wb'))\n"", '\n', ""#plt.rc('text', usetex=True, color='black')\n"", ""#plt.rc('font', family='serif')\n"", '\n', 'fig = plt.figure(figsize=(12, 8))\n', 'gs = matplotlib.gridspec.GridSpec(2, 2) #, width_ratios=[1,0.75,1])\n', '\n', 'ax1 = fig.add_subplot(gs[0])\n', 'ax2 = fig.add_subplot(gs[1], sharey=ax1)\n', 'ax3 = fig.add_subplot(gs[2])\n', 'ax4 = fig.add_subplot(gs[3], sharey=ax3)\n', '   \n', ""data_motion.plot(x='ga',y='Left lung',kind='scatter',ax=ax1, color=np.array([255, 0, 0])/255,marker='o',edgecolor='black',linewidth=1.2,alpha=.5);\n"", ""data_motion.plot(x='ga',y='Right lung',kind='scatter',ax=ax2, color=np.array([0, 255, 63])/255,marker='o',edgecolor='black',linewidth=1.2,alpha=.5);\n"", ""data_motion.plot(x='ga',y='Heart',kind='scatter',ax=ax3, color=np.array([0, 0, 255])/255,marker='o',edgecolor='black',linewidth=1.2,alpha=.5);\n"", ""data_motion.plot(x='ga',y='Liver',kind='scatter',ax=ax4, color=np.array([255, 255, 0])/255,marker='o',edgecolor='black',linewidth=1.2,alpha=.5);\n"", '\n', 'ax1.set_ylim(0,80)\n', 'ax2.set_ylim(0,80)\n', 'ax3.set_ylim(0,80)\n', 'ax4.set_ylim(0,80)\n', '\n', ""obj = ax1.set_title('Left lung')\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax2.set_title('Right lung')\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax3.set_title('Heart')\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax4.set_title('Liver')\n"", ""plt.setp(obj, color='black')\n"", '\n', ""obj = ax1.set_ylabel('Distance error (mm)')\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax3.set_ylabel('Distance error (mm)')\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax2.set_ylabel('')\n"", ""obj = ax4.set_ylabel('')\n"", '\n', ""obj = ax3.set_xlabel('Gestational age')\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax4.set_xlabel('Gestational age')\n"", ""plt.setp(obj, color='black')\n"", '\n', 'fig.tight_layout()\n', '\n', '#plt.savefig(""detection_by_ga.pdf"", bbox_inches=""tight"")']",EXPLORE,EVALUATE
1578,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1109134.ipynb,1109134,20,"[""plt.rc('text', usetex=True, color='black')\n"", ""plt.rc('font', family='serif')\n"", '\n', 'fig = plt.figure(figsize=(15, 4.5))\n', 'gs = matplotlib.gridspec.GridSpec(1, 3, width_ratios=[0.8,0.8,1])\n', '\n', 'ax1 = fig.add_subplot(gs[0])\n', 'ax2 = fig.add_subplot(gs[1], sharey=ax1)\n', 'ax3 = fig.add_subplot(gs[2], sharey=ax1)\n', '\n', 'label_names = [""Left\\nlung"",""Right\\nlung"",""Heart"",""Liver""]\n', '\n', '## Healthy\n', 'data_leaveoneout[data_leaveoneout[\'IUGR\']==0][[""Left lung"",""Right lung"",""Heart"",""Liver""]].boxplot(return_type=\'axes\',\n', ""                                                                               flierprops={'marker':'.','color':[0,0,0]},\n"", '                                                                               fontsize=18,\n', '                                                                               ax=ax1);\n', '\n', 'ax1.set_ylim([0,40])\n', 'ax1.yaxis.set_ticks(np.arange(0, 41, 5))\n', 'obj = ax1.set_ylabel(""Distance error (mm)"")\n', ""plt.setp(obj, color='black')\n"", ""#obj = ax1.set_title(r'1\\textsuperscript{st} dataset: healthy');\n"", ""obj = ax1.set_title(r'Dataset 2: healthy');\n"", ""plt.setp(obj, color='black')\n"", 'obj = ax1.set_xticklabels(label_names)\n', ""plt.setp(obj, color='black')\n"", '\n', ""axes_obj = plt.getp(ax1,'axes')                 #get the axes' property handler\n"", ""ytl_obj = plt.getp(axes_obj, 'yticklabels')     #get the properties for \n"", '                                                #  yticklabels\n', ""plt.setp(ytl_obj, color='black')\n"", '\n', '## IUGR\n', 'data_leaveoneout[data_leaveoneout[\'IUGR\']==1][[""Left lung"",""Right lung"",""Heart"",""Liver""]].boxplot(return_type=\'axes\',\n', ""                                                                               flierprops={'marker':'.','color':[0,0,0]},\n"", '                                                                               fontsize=18,\n', '                                                                               ax=ax2);\n', ""#obj = ax2.set_title(r'1\\textsuperscript{st} dataset: IUGR');\n"", ""obj = ax2.set_title(r'Dataset 2: IUGR');\n"", ""plt.setp(obj, color='black')\n"", 'obj = ax2.set_xticklabels(label_names)\n', ""plt.setp(obj, color='black')\n"", '\n', ""axes_obj = plt.getp(ax2,'axes')                 #get the axes' property handler\n"", ""ytl_obj = plt.getp(axes_obj, 'yticklabels')     #get the properties for \n"", '                                                #  yticklabels\n', ""plt.setp(ytl_obj, color='black')\n"", '\n', '## Motion artefacts\n', 'data_motion[[""Left lung"",""Right lung"",""Heart"",""Liver"",""Brain""]].boxplot(return_type=\'axes\',\n', ""                                                                      flierprops={'marker':'.','color':[0,0,0]},\n"", '                                                                      fontsize=18,\n', '                                                                      ax=ax3);\n', ""#obj = ax3.set_title(r'2\\textsuperscript{nd} dataset');\n"", ""obj = ax3.set_title(r'Dataset 3');\n"", ""plt.setp(obj, color='black')\n"", ""obj = ax3.set_xticklabels(label_names+['Brain'])\n"", ""plt.setp(obj, color='black')\n"", '\n', ""axes_obj = plt.getp(ax3,'axes')                 #get the axes' property handler\n"", ""ytl_obj = plt.getp(axes_obj, 'yticklabels')     #get the properties for \n"", '                                                #  yticklabels\n', ""plt.setp(ytl_obj, color='black')\n"", '\n', 'plt.savefig(""combined_plot2.pdf"", bbox_inches=""tight"");']",EXPLORE,EVALUATE
1176,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1116374.ipynb,1116374,38,"['month_occurrence = blitz.groupby(""Month"").groups\n', 'print(len(month_occurrence[1]))\n', '\n', 'occurrence = []\n', '\n', 'for i in range(1,13):\n', '    val = month_occurrence[i]\n', '    occurrence.append(len(val))\n', '\n', 'print(occurrence)\n', 'print(sum(occurrence))']",WRANGLE,EXPLORE
1181,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1116374.ipynb,1116374,44,"['np.corrcoef(blitz[""Magnitude""], blitz[""Depth""])']",EVALUATE,EXPLORE
1186,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1116374.ipynb,1116374,56,"[""print('Coefficients: \\n', regr.coef_)""]",EXPLORE,EVALUATE
1187,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1116374.ipynb,1116374,58,['Y2 = regr.predict(X1)'],MODEL,EVALUATE
602,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1118399.ipynb,1118399,34,"[""#add a date field to map the week number for each track to the week of the year it's actually on the track\n"", 'bd_melt[""rank_on_chart""] = [101 - x for x in bd_melt.Ranking]\n', 'bd_melt.info()']",EXPLORE,WRANGLE
853,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1138666.ipynb,1138666,29,"['import sqlite3 as lite\n', '\n', ""con = lite.connect('citi_bike.db')\n"", 'cur = con.cursor()\n', '\n', 'with con:\n', ""    cur.execute('CREATE TABLE citibike_reference (id INT PRIMARY KEY, totalDocks INT, city TEXT, altitude INT, stAddress2 TEXT, longitude NUMERIC, postalCode TEXT, testStation TEXT, stAddress1 TEXT, stationName TEXT, landMark TEXT, latitude NUMERIC, location TEXT )')""]",EXPLORE,WRANGLE
1718,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1143058.ipynb,1143058,8,"['examples = [\'Free Viagra now!!!\', ""Hi Bob, how about a game of golf tomorrow?""]\n', 'example_counts = vectorizer.transform(examples)\n', 'predictions = classifier.predict(example_counts)\n', 'predictions']",MODEL,EVALUATE
770,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1149480.ipynb,1149480,14,"['max_steps = 200\n', 'print_every = max_steps / 10\n', 'alpha = 0.0001  # learning rate\n', '\n', ""# We'll constrain this to run on CPU, as GPUs aren't very efficient\n"", '# on a model this small.\n', 'session = tf.Session(config=tf.ConfigProto(device_filters=""/cpu:0""))\n', 'session.run(init_)  # initialize variables for this session\n', '\n', 't0 = time.time()\n', 'for i in xrange(max_steps):          \n', '    # Run a single gradient descent step\n', '    c, p, _ = session.run([loss_, pred_proba_, train_step_],\n', '                           feed_dict={X_: X, y_: y, alpha_: alpha})\n', '        \n', '    if (i % print_every == 0):\n', '        avg_cost = sum(c) / len(y)\n', '        print ""[iter %d] Average cost: %.03f"" % (i, avg_cost)\n', '        \n', 'print ""Training completed in %.02f s"" % (time.time() - t0)\n', '    \n', '# Run model over training set one last time\n', 'costs, y_pred = session.run([loss_, pred_proba_], \n', '                            feed_dict={X_: X, y_: y})\n', '\n', 'print """"\n', 'print ""Accuracy: %.03f"" % np.mean(y == (y_pred >= 0.5))\n', 'print ""Mean cross-entropy loss: %.03f"" % np.mean(costs)\n', '\n', 'fig = plt.figure()\n', 'plot_model(X, y, lambda X: session.run(pred_proba_, feed_dict={X_:X}))\n', 'plt.title(""%d steps (final)"" % max_steps)']",EVALUATE,MODEL
773,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1149480.ipynb,1149480,21,"['summary_writer = tf.train.SummaryWriter(""tf_summaries"", session.graph)']",EVALUATE,MODEL
1361,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,0,"['import pyes\n', 'import json\n', '\n', ""host = '140.118.155.14:9200'\n"", '\n', ""fid1 = '143039229176602'\n"", ""fid2 = '119474188105563'\n"", '\n', 'def fans(fid):\n', '    conn = pyes.es.ES(host)\n', '    tq = pyes.query.TermQuery(field=""fid"", value=fid)\n', '    qsearch = pyes.query.Search(tq) \n', ""    result = conn.search(query=qsearch , indices='facebook_nested' , doc_types='fanpage')\n"", '    for fidpid_summary in result:\n', ""        count = fidpid_summary['likes_count']\n"", ""        print fid + ',' + str(count)\n"", '        count = int(count)\n', '        return count\n', '        \n', 'fans(fid1)\n', 'fans(fid2)\n', '\n', 'FanpageLikes = [753040, 464181]']",IMPORT,WRANGLE
1364,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,3,"['def sharingPerPosts(fid):\n', '    conn = pyes.es.ES(host)\n', '    tq = pyes.query.TermQuery(field=""fid"", value=fid)\n', '    qsearch = pyes.query.Search(tq) \n', ""    result = conn.search(query=qsearch , indices='facebook_nested' , doc_types='post')\n"", '    for fidpid_summary in result:\n', '        try:\n', ""            print fid + ',' + str(fidpid_summary['shares']['count'])\n"", '        except KeyError:\n', ""            print fid + ',0'\n"", '\n', 'sharingPerPosts(fid1)']",EXPLORE,WRANGLE
1365,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,4,"['def bb(fid):\n', '    conn = pyes.es.ES(host)\n', '    tq = pyes.query.TermQuery(field=""fid"", value=fid)\n', ""    #tagg = pyes.aggs.TermsAgg('pid', field= 'fid_pid')\n"", ""    DHAgg = pyes.aggs.DateHistogramAgg('month' ,field='created_time', interval='month',sub_aggs=[])\n"", ""    tagg = pyes.aggs.TermsAgg('pid', field= 'fid_pid')\n"", '    DHAgg.sub_aggs.append(tagg) \n', '    qsearch = pyes.query.Search(tq) \n', '    qsearch.agg.add(DHAgg) \n', ""    result = conn.search(query=qsearch , indices='facebook_nested' , doc_types='comment')\n"", '    print json.dumps(result.aggs,indent=2) \n', 'bb(fid2)']",EXPLORE,WRANGLE
1366,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,5,"['import pandas as pd\n', 'import seaborn as sns\n', 'import datetime\n', 'import matplotlib.pyplot as plt\n', '\n', '\n', 'def NumofPostPerMonth(fid,host,lowerbound,upperbound):\n', '    conn = pyes.es.ES(host)\n', '    tq = pyes.query.TermQuery(field=""fid"", value=fid)\n', ""    DHAgg = pyes.aggs.DateHistogramAgg('month' ,field='created_time', interval='month',sub_aggs=[])\n"", ""    tagg = pyes.aggs.TermsAgg('pid', field= 'fid_pid')\n"", '    DHAgg.sub_aggs.append(tagg) \n', '    qsearch = pyes.query.Search(tq) \n', '    qsearch.agg.add(DHAgg) \n', ""    result = conn.search(query=qsearch , indices='facebook_nested' , doc_types='comment')\n"", '\n', '    data=[]\n', '    for r in result.aggs[""month""][""buckets""]:\n', '        for pid in r[""pid""][""buckets""]:\n', '            monthyear=datetime.datetime.strptime(r[""key_as_string""], ""%Y-%m-%dT%H:%M:%S.%fZ"").strftime(\'%Y/%m\')\n', '            data.append([pid[""doc_count""],monthyear,pid[""key""],r[""key""]])\n', '\n', ""    df=pd.DataFrame(data,columns={'date','count','fid','pid'})\n"", '    print data\n', '\n', ""print 'NumofPostPerMonthFid1'\n"", 'NumofPostPerMonth(fid1,host,0,500)\n', ""print 'NumofPostPerMonthFid2'\n"", 'NumofPostPerMonth(fid2,host,0,500)']",EXPLORE,WRANGLE
1367,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,6,"['import pyes\n', 'import elasticsearch\n', 'import datetime\n', 'conn = pyes.es.ES(host)\n', 'bq = pyes.query.BoolQuery()\n', '\n', 'tq = pyes.query.TermQuery(field=""fid"", value=fid1)\n', 'tq1 = pyes.query.TermQuery(field=""message"", value=""抽"")\n', 'bq.add_must(tq)\n', 'bq.add_should(tq1)\n', ""result = conn.search(query=bq , indices='facebook_nested' , doc_types='post')\n"", '\n', 'CountFid1 = len(result)\n', ""print fid1 + ',' + str(CountFid1)""]",EXPLORE,WRANGLE
1368,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,7,"['bq = pyes.query.BoolQuery()\n', '\n', 'tq = pyes.query.TermQuery(field=""fid"", value=fid2)\n', 'tq1 = pyes.query.TermQuery(field=""message"", value=""抽"")\n', 'bq.add_must(tq)\n', 'bq.add_should(tq1)\n', ""result = conn.search(query=bq , indices='facebook_nested' , doc_types='post')\n"", '\n', 'CountFid2 = len(result)\n', ""print fid2 + ',' + str(CountFid2)""]",EXPLORE,WRANGLE
1379,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,18,"['import numpy as np\n', '\n', 'FIDS2 = [143039229176602,119474188105563]\n', ""PostID = ['143039229176602_557278391086015',\n"", ""          '143039229176602_525569317590256',\n"", ""          '143039229176602_527394674074387',\n"", ""          '143039229176602_529428137204374',\n"", ""          '143039229176602_579364662210721',\n"", ""          '143039229176602_211266805687177',\n"", ""          '143039229176602_595162223964298',\n"", ""          '143039229176602_478371565643365',\n"", ""          '143039229176602_431631056984083',\n"", ""          '143039229176602_315673121913211',\n"", ""          '119474188105563_1150657671653871',\n"", ""          '119474188105563_1114048741981431',\n"", ""          '119474188105563_968598386526468',\n"", ""          '119474188105563_485312648188380',\n"", ""          '119474188105563_482836958435949',\n"", ""          '119474188105563_955817441137896',\n"", ""          '119474188105563_1057866434266329',\n"", ""          '119474188105563_825288854190756',\n"", ""          '119474188105563_846198005433174',\n"", ""          '119474188105563_881871541865820']\n"", '#1:與抽獎有關貼文\n', '#0:一般貼文\n', 'DataMiningLabel = [1, 1, 0, 0, 1, 1, 1, 1, 0, 0, \n', '                   0, 1, 1, 0, 0, 0, 1, 0, 1, 0]\n', '\n', '# F1:Likes>1000  F2:Sharing<100  F3:Comments>200  F4:發文時間在13:00~24:00 F5:發文是附影片\n', 'DataMining = np.array([[1, 0, 1, 0, 0],\n', '                       [1, 0, 1, 0, 1],\n', '                       [1, 1, 0, 1, 1],\n', '                       [0, 1, 0, 0, 0],\n', '                       [1, 0, 1, 0, 0],\n', '                       [1, 0, 1, 1, 1],\n', '                       [1, 0, 1, 1, 0],\n', '                       [1, 0, 1, 0, 1],\n', '                       [1, 0, 0, 0, 0],\n', '                       [1, 1, 0, 1, 0],\n', '                       [1, 1, 0, 0, 0],\n', '                       [1, 0, 1, 0, 0],\n', '                       [1, 0, 1, 1, 1],\n', '                       [0, 1, 0, 0, 0],\n', '                       [0, 1, 0, 1, 0],\n', '                       [1, 0, 1, 0, 0],\n', '                       [1, 0, 1, 0, 1],\n', '                       [0, 1, 0, 1, 1],\n', '                       [1, 1, 1, 1, 0],\n', '                       [0, 1, 0, 1, 1]])\n', '\n', 'print DataMining']",EXPLORE,WRANGLE
1381,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,20,"['%matplotlib inline\n', 'import seaborn as sb\n', '\n', 'model_accuracies = []\n', '\n', 'for repetition in range(1000):\n', '    (training_inputs,\n', '     testing_inputs,\n', '     training_classes,\n', '     testing_classes) = train_test_split(DataMining, DataMiningLabel, train_size=0.75)\n', '    \n', '    decision_tree_classifier = DecisionTreeClassifier()\n', '    decision_tree_classifier.fit(training_inputs, training_classes)\n', '    classifier_accuracy = decision_tree_classifier.score(testing_inputs, testing_classes)\n', '    model_accuracies.append(classifier_accuracy)\n', '    \n', 'sb.distplot(model_accuracies)']",EVALUATE,MODEL
1383,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1159388.ipynb,1159388,22,"['import matplotlib.pyplot as plt\n', 'grid_visualization = []\n', '\n', 'for grid_pair in grid_search.grid_scores_:\n', '    grid_visualization.append(grid_pair.mean_validation_score)\n', '    \n', 'grid_visualization = np.array(grid_visualization)\n', 'grid_visualization.shape = (5, 5)\n', ""sb.heatmap(grid_visualization, cmap='Blues')\n"", ""plt.xticks(np.arange(5) + 0.5, grid_search.param_grid['max_features'])\n"", ""plt.yticks(np.arange(5) + 0.5, grid_search.param_grid['max_depth'][::-1])\n"", ""plt.xlabel('max_features')\n"", ""plt.ylabel('max_depth')""]",EXPLORE,EVALUATE
285,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1161031.ipynb,1161031,3,"['import numpy as np\n', 'import pandas as pd\n', '\n', '# RMS Titanic data visualization code \n', '# 数据可视化代码\n', 'from titanic_visualizations import survival_stats\n', 'from IPython.display import display\n', '%matplotlib inline\n', '\n', '# Load the dataset \n', '# 加载数据集\n', ""in_file = 'titanic_data.csv'\n"", 'full_data = pd.read_csv(in_file)\n', '\n', '# Print the first few entries of the RMS Titanic data \n', '# 显示数据列表中的前几项乘客数据\n', 'display(full_data.head())']",WRANGLE,IMPORT
103,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,8,"[""#nltk.tokenize.word_tokenize(train_data[0]['reviewText'])\n"", ""nltk.tokenize.sent_tokenize(train_data[0]['reviewText'])""]",EXPLORE,WRANGLE
108,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,13,"['# baseline score - all positive\n', 'print(""baseline train score, all positive class"")\n', 'print(""train:"")\n', 'print(score_batch([1]*len(train_targets), train_targets))\n', 'print(""dev:"")\n', 'print(score_batch([1]*len(dev_targets), dev_targets))\n', 'print(""test:"")\n', 'print(score_batch([1]*len(test_targets), test_targets))\n', '\n', 'print(""baseline train score, all negative class"")\n', 'print(""train:"")\n', 'print(score_batch([0]*len(train_targets), train_targets))\n', 'print(""dev:"")\n', 'print(score_batch([0]*len(dev_targets), dev_targets))\n', 'print(""test:"")\n', 'print(score_batch([0]*len(test_targets), test_targets))']",EXPLORE,EVALUATE
111,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,16,"['# save works\n', 'import pickle\n', ""with open('train_data_pos.p', 'wb') as f:\n"", '    pickle.dump(train_data, f)\n', ""with open('test_data_pos.p', 'wb') as f:\n"", '    pickle.dump(test_data, f)\n', ""with open('dev_data_pos.p', 'wb') as f:\n"", '    pickle.dump(dev_data, f)']",EXPLORE,WRANGLE
112,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,17,"['# load data\n', 'import pickle\n', ""with open('train_data_pos.p', 'rb') as f:\n"", '    train_data = pickle.load(f)\n', ""with open('test_data_pos.p', 'rb') as f:\n"", '    test_data = pickle.load(f)\n', ""with open('dev_data_pos.p', 'rb') as f:\n"", '    dev_data = pickle.load(f)']",EXPLORE,WRANGLE
113,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,18,"['import collections\n', ""pos_tag_dist = collections.Counter((pos[1] for pos in d['reviewTextPOS'] for d in train_data))""]",EXPLORE,WRANGLE
118,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,23,"['# feature functions\n', 'def typeToTokenRatio(data):\n', ""    tokens = data['reviewWordTokens']\n"", '    types = set([t.lower() for t in tokens])\n', '    return len(types)*1.0/len(tokens)\n', '\n', 'def meanWordLength(data):\n', ""    wordlengths = [len(t) for t in data['reviewWordTokens'] if t not in string.punctuation]\n"", '    return sum(wordlengths)*1.0/len(wordlengths)\n', '\n', 'def punctuationRatio(data):\n', '    puncs = 0.0\n', ""    for t in data['reviewWordTokens']:\n"", '        if t in string.punctuation:\n', '            puncs += 1\n', ""    return puncs/len(data['reviewWordTokens'])\n"", '\n', 'def posRatio(data, target_pos):\n', '    counts = 0.0\n', ""    for pos in data['reviewTextPOS']:\n"", '        if target_pos == pos[1]:\n', '            counts += 1\n', ""    return counts/len(data['reviewTextPOS'])""]",WRANGLE,MODEL
119,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,24,"['model.registerFeature(""log text length"", lambda d: np.log(len(d[\'reviewText\'])))\n', 'model.registerFeature(""type-token ratio"", typeToTokenRatio)\n', 'model.registerFeature(""mean word length"", meanWordLength)\n', 'model.registerFeature(""mean sent length"", lambda d: np.mean([len(s) for s in d[\'reviewSentences\']]))\n', 'model.registerFeature(""sent length stdev"", lambda d: np.std([len(s) for s in d[\'reviewSentences\']]))\n', 'model.registerFeature(""punctuation ratio"", punctuationRatio)\n', 'model.registerFeature(""punctuation ratio"", punctuationRatio)\n', ""for target_pos in ['CC','CD','DT','EX','IN','JJ','JJR',\n"", ""    'JJS','MD','NN','NNP','NNS','POS',\n"", ""    'PRP','PRP$','RB','RBR','RBS','RP',\n"", ""    'TO','VB','VBD','VBG','VBN','VBP',\n"", ""    'VBZ','WRB']:\n"", '    model.registerFeature(""POS %s ratio""%target_pos, lambda d: posRatio(d, target_pos))']",WRANGLE,MODEL
121,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,26,"['train_data_feaurized = model.makeData(train_data, standardize=True)\n', 'dev_data_feaurized = model.makeData(dev_data, standardize=True)\n', 'test_data_feaurized = model.makeData(test_data, standardize=True)']",WRANGLE,MODEL
126,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,31,"['sorted(model.coef(), key=lambda k: abs(k[1]), reverse=True)']",EXPLORE,EVALUATE
130,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,36,"['for r in batchGenerator(train_data_feaurized[:2], train_targets[:2], 1):\n', '    print r']",WRANGLE,EXPLORE
132,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,38,"['# get test score\n', ""trained_filename = 'tf_saved/nn_with_manual_features'\n"", 'batch_size = 5\n', 'learning_rate = 0.1\n', 'num_epochs = 100\n', '\n', 'with tf.Graph().as_default(), tf.Session() as session:\n', '    with tf.variable_scope(""model"", reuse=None):\n', '        lm = TextModel()\n', '        session.run(tf.initialize_all_variables())\n', '        saver = tf.train.Saver()\n', '    saver = tf.train.Saver()\n', '    saver.restore(session, trained_filename)\n', '    \n', '    print(""test score"")\n', '    bi = batchGenerator(test_data_feaurized,test_targets, len(test_data))\n', '    for (w,y) in bi:\n', '        feed_dict = { lm.input_w_:w}\n', '        pred_prob = session.run(lm.pred_proba_, feed_dict)\n', '        print(score_batch(pred_prob, y))']",MODEL,EVALUATE
133,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,39,"[""trained_filename = 'tf_saved/nn_with_manual_features_2'\n"", 'batch_size = 5\n', 'learning_rate = 0.1\n', 'num_epochs = 40\n', 'runTraining()']",MODEL,WRANGLE
134,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163520.ipynb,1163520,40,"['# get test score\n', ""trained_filename = 'tf_saved/nn_with_manual_features_2'\n"", '\n', 'with tf.Graph().as_default(), tf.Session() as session:\n', '    with tf.variable_scope(""model"", reuse=None):\n', '        lm = TextModel()\n', '        session.run(tf.initialize_all_variables())\n', '        saver = tf.train.Saver()\n', '    saver = tf.train.Saver()\n', '    saver.restore(session, trained_filename)\n', '    \n', '    print(""test score"")\n', '    bi = batchGenerator(test_data_feaurized,test_targets, len(test_data))\n', '    for (w,y) in bi:\n', '        feed_dict = { lm.input_w_:w}\n', '        pred_prob = session.run(lm.pred_proba_, feed_dict)\n', '        print(score_batch(pred_prob, y))']",MODEL,EVALUATE
399,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1163801.ipynb,1163801,5,['df1 = df.copy()'],WRANGLE,EXPLORE
1709,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1170705.ipynb,1170705,6,"['doctopic = (doctopic / (np.sum(doctopic, axis=1, keepdims=True)))']",WRANGLE,EVALUATE
1710,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1170705.ipynb,1170705,7,"['#promedio de las acciones de temas asociados con el mismo documento\n', 'doc_names = []\n', '\n', 'for fn in filenames:\n', '    basename = os.path.basename(fn)\n', '    name, ext = os.path.splitext(basename)\n', ""    name = name.rstrip('0123456789')\n"", '    doc_names.append(name) \n', '\n', '# Convertir en matriz para que podamos usar las funciones NumPy \n', 'doc_names = np.asarray(doc_names)\n', 'doctopic_orig = doctopic.copy()\n', '\n', '# use method described in preprocessing section\n', 'num_groups = len(set(doc_names))\n', 'doctopic_grouped = np.zeros((num_groups, num_topics))#721,20\n', '\n', 'for i, name in enumerate(sorted(set(doc_names))):\n', '    doctopic_grouped[i, :] = np.mean(doctopic[doc_names == name, :], axis=0) \n', '\n', 'doctopic = doctopic_grouped']",WRANGLE,EVALUATE
1711,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1170705.ipynb,1170705,8,"['# Vamos a identificar los temas más importantes para cada texto en el corpus.\n', '# Este procedimiento no difiere esencialmente del procedimiento de identificación\n', '# de las palabras más frecuentes en cada texto.7\n', 'docs = sorted(set(doc_names))\n', 'print(""Top NMF topics in..."")\n', '\n', '# Tema en los que clasifica cada doc del corpus\n', 'for i in range(len(doctopic)):\n', '    top_topics = np.argsort(doctopic[i,:])[::-1][0:]\n', '    #top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n', ""    top_topics_str = ' '.join(str(t) for t in top_topics)\n"", '    print(""{}: {}"".format(docs[i], top_topics_str))']",WRANGLE,EXPLORE
1712,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1170705.ipynb,1170705,9,"['# Tenemos listas de palabras (topic_words) que más se relacionan con los componentes.\n', 'for t in range(len(topic_words)):\n', '    print(""Topic {}: {}"".format(t, \' \'.join(topic_words[t][:15])))']",WRANGLE,EXPLORE
1713,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1170705.ipynb,1170705,10,"['# Temas distintivos\n', 'video_indices, noticia_indices = [], []\n', '\n', 'for index, fn in enumerate(sorted(set(doc_names))):\n', '    if ""video"" in fn:\n', '        video_indices.append(index)\n', '    elif ""noticia"" in fn:\n', '        noticia_indices.append(index)\n', '\n', 'video_avg = np.mean(doctopic[video_indices, :], axis=0)\n', 'noticia_avg = np.mean(doctopic[noticia_indices, :], axis=0)\n', 'keyness = np.abs(video_avg - noticia_avg)\n', 'ranking = np.argsort(keyness)[::-1]  # from highest to lowest; [::-1] reverses order in Python sequences\n', '\n', '# distinctive topics:\n', 'ranking[:10]']",WRANGLE,EVALUATE
1714,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1170705.ipynb,1170705,11,"['%matplotlib inline\n', 'import matplotlib.pyplot as plt\n', '\n', 'N, K = doctopic.shape\n', 'ind = np.arange(N)  # points on the x-axis\n', '\n', 'width = 0.5\n', '\n', 'plt.bar(ind, doctopic[:,0], width=width)\n', 'plt.xticks(ind + width/2, docs)  # put labels in the center\n', '\n', ""plt.title('Share of Topic #0')""]",EXPLORE,EVALUATE
1392,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1172355.ipynb,1172355,8,"['new_series2[2]=5\n', 'new_series2']",EXPLORE,WRANGLE
1395,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1172355.ipynb,1172355,11,"['import numpy as np\n', 'np.exp(new_series2)']",EXPLORE,IMPORT
1397,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1172355.ipynb,1172355,13,"[""data={'Data Science':90000,'Data Analyst':80000}\n"", 's3=Series(data)\n', 's3']",EXPLORE,WRANGLE
391,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1180754.ipynb,1180754,8,"['from scipy.stats import gaussian_kde\n', 'vstacked = np.vstack([hamlet_larvae.Hamlet, hamlet_larvae.Larvae_count])\n', 'density_color = gaussian_kde(vstacked)(vstacked)\n', 'idx = density_color.argsort()\n', 'hamlet_larvae.Hamlet = hamlet_larvae.Hamlet[idx]\n', 'hamlet_larvae.Larvae_count = hamlet_larvae.Larvae_count[idx]\n', 'density_color = density_color[idx]']",WRANGLE,MODEL
1623,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1183781.ipynb,1183781,8,"['def mae(y_true, y_pred):\n', '    return (abs(y_true - y_pred)).mean(axis=1).mean()']",MODEL,EVALUATE
1628,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1183781.ipynb,1183781,15,"['predictions = ns.predict()\n', ""print 'Mean Absolute Error %f ' %(mae(target, predictions))""]",MODEL,EVALUATE
1629,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1183781.ipynb,1183781,17,['diff_in_predictions = target - predictions'],WRANGLE,EVALUATE
1679,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1183943.ipynb,1183943,5,"['data = pd.read_csv(""HR.csv"", header=0, sep=\';\')\n', 'data.loc[data.salary==""low"",""salary""] = 1\n', 'data.loc[data.salary==""medium"",""salary""] = 2\n', 'data.loc[data.salary==""high"",""salary""] = 3\n', 'data.salary = data.salary.astype(""int64"")\n', 'print ""Overall amount of employees"", data.shape[0]\n', 'print ""That employees who had left company"", data[data.left==1].shape[0]\n']",EXPLORE,WRANGLE
1686,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1183943.ipynb,1183943,28,"['valuable_employees = data[(data.last_evaluation>=80) | \\\n', '                          (data.number_project>=5) | \\\n', '                          (data.average_montly_hours>=220)]\n', 'print ""Number of valuable employees: "", valuable_employees.shape[0]\n', 'print ""Number of valuable employees that left the company: "", valuable_employees[valuable_employees.left==1].shape[0]\n', 'res = int(100*valuable_employees[valuable_employees.left==1].shape[0]/float(valuable_employees.shape[0]))\n', 'print str(res)+""%"", ""of them have left the company""']",EXPLORE,WRANGLE
615,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1184211.ipynb,1184211,9,"['# here we define some important hyperparameter of the NN\n', 'batch_size = 128\n', 'nb_classes = 10\n', 'nb_epoch = 6\n', 'img_rows, img_cols = 28, 28\n', 'kernel_size = (3, 3)\n', 'input_shape = (img_rows, img_cols, 1)\n', 'pool_size = (2, 2)']",MODEL,WRANGLE
620,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1184211.ipynb,1184211,14,"['# summarize history for accuracy\n', ""plt.plot(history.history['acc'])\n"", ""plt.plot(history.history['val_acc'])\n"", ""plt.title('model accuracy')\n"", ""plt.ylabel('accuracy')\n"", ""plt.xlabel('epoch')\n"", ""plt.legend(['train', 'valid'], loc='lower right')\n"", 'plt.show()']",EVALUATE,EXPLORE
621,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1184211.ipynb,1184211,15,"[""plt.plot(history.history['loss'])\n"", ""plt.plot(history.history['val_loss'])\n"", ""plt.title('model loss')\n"", ""plt.ylabel('loss')\n"", ""plt.xlabel('epoch')\n"", ""plt.legend(['train', 'valid'], loc='upper right')\n"", 'plt.show()']",EVALUATE,EXPLORE
985,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1193256.ipynb,1193256,13,"['def male_female_child(passenger):\n', '    age, sex = passenger\n', '    if age < 16:\n', ""        return 'child'\n"", '    else:\n', '        return sex']",WRANGLE,MODEL
998,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1193256.ipynb,1193256,26,['levels = [level[0] for level in deck]'],WRANGLE,EXPLORE
1003,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1193256.ipynb,1193256,31,"[""cabin_df = cabin_df[cabin_df.Cabin != 'T']""]",EXPLORE,WRANGLE
1223,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1198274.ipynb,1198274,0,"['import pandas as pd\n', 'from random import random\n', '%matplotlib inline\n', '%load_ext autoreload\n', '%autoreload 2\n', '\n', 'flow = (list(range(1,10,1)) + list(range(10,1,-1)))*100\n', 'pdata = pd.DataFrame({""a"":flow, ""b"":flow})\n', 'pdata.b = pdata.b.shift(9)\n', 'data = pdata.iloc[10:] * random()  # some noise\n', 'data[0:50].plot()']",IMPORT,WRANGLE
795,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1202205.ipynb,1202205,2,"['import pandas as pd\n', 'import pandas_datareader as web\n', 'import datetime\n', 'import matplotlib.pyplot as plt   \n', '\n', '%matplotlib inline\n', '%pylab inline\n', ""pylab.rcParams['figure.figsize'] = (18, 10) # Change the size of plots\n"", '\n', 'start = datetime.datetime(2017,1,1)\n', 'end = datetime.date.today()\n', '\n', '# First argument is the stock ticker we want, second is the source (""yahoo"" for Yahoo! Finance), third is the start date, fourth is the end date\n', 'amazon = web.DataReader(""AMZN"", ""yahoo"", start, end)']",IMPORT,WRANGLE
798,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1202205.ipynb,1202205,6,"[""# Let's do the same with fedex\n"", 'fedex = web.DataReader(""FDX"", ""yahoo"", start, end)\n', 'fedex.head(3)']",EXPLORE,WRANGLE
799,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1202205.ipynb,1202205,7,"['# And UPS\n', 'ups = web.DataReader(""UPS"", ""yahoo"", start, end)\n', 'ups.head(3)']",EXPLORE,WRANGLE
819,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1202205.ipynb,1202205,43,"['# official doc\n', 'from IPython.core.display import HTML\n', 'HTML(""<iframe src=https://www.crummy.com/software/BeautifulSoup/ width=800 height=350></iframe>"")']",EXPLORE,IMPORT
822,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1202205.ipynb,1202205,48,"['from IPython.display import YouTubeVideo\n', ""YouTubeVideo('qyt7Rnpk8lM')""]",MODEL,IMPORT
696,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1203216.ipynb,1203216,7,"['%matplotlib inline\n', '\n', 'X, labels = genBlobs(centers=5)\n', 'mu, sigma = mlParams(X,labels)\n', 'plotGaussian(X,labels,mu,sigma)']",WRANGLE,EVALUATE
1100,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1212287.ipynb,1212287,5,"['#there appears to be one significant outlier\n', 'subsurvey.dropna()']",EXPLORE,WRANGLE
1103,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1212287.ipynb,1212287,9,"['#average expenditure and kwh per household per month\n', '#groupby access type shown to verify grid type\n', ""household_mean = survey.groupby(['access_type'])[['PLN_expenditure_monthly','Ajau_kWh']].mean()\n"", 'household_mean.reset_index(inplace=True)\n', 'household_mean']",EXPLORE,WRANGLE
1506,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,15,"['from sklearn.linear_model import LogisticRegression\n', 'scores = cross_val_score(LogisticRegression(),\n', ""                         X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(scores.mean())']",MODEL,EVALUATE
1507,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,16,"['from sklearn.linear_model import LogisticRegressionCV\n', ""scores = cross_val_score(LogisticRegressionCV(scoring='roc_auc'), X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(scores.mean())']",MODEL,EVALUATE
1508,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,18,"['from imblearn.under_sampling import RandomUnderSampler\n', '\n', 'rus = RandomUnderSampler(replacement=False)\n', 'X_train_subsample, y_train_subsample = rus.fit_sample(X_train, y_train)\n', 'print(X_train.shape)\n', 'print(X_train_subsample.shape)\n', 'print(np.bincount(y_train_subsample))']",WRANGLE,MODEL
1509,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,19,"['from imblearn.pipeline import make_pipeline as make_imb_pipeline\n', '\n', 'undersample_pipe = make_imb_pipeline(RandomUnderSampler(), LogisticRegressionCV())\n', ""scores = cross_val_score(undersample_pipe, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1511,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,21,"['oversample_pipe = make_imb_pipeline(RandomOverSampler(), LogisticRegression())\n', ""scores = cross_val_score(oversample_pipe, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1513,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,23,"['from sklearn.ensemble import RandomForestClassifier\n', 'scores = cross_val_score(RandomForestClassifier(n_estimators=100),\n', ""                         X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1514,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,24,"['undersample_pipe_rf = make_imb_pipeline(RandomUnderSampler(), RandomForestClassifier())\n', ""scores = cross_val_score(undersample_pipe_rf, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1515,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,25,"['oversample_pipe_rf = make_imb_pipeline(RandomOverSampler(), RandomForestClassifier())\n', ""scores = cross_val_score(oversample_pipe_rf, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1516,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,26,"['from sklearn.metrics import roc_curve\n', 'oversample_pipe_rf.fit(X_train, y_train)\n', 'props_oversample = oversample_pipe_rf.predict_proba(X_test)[:, 1]\n', 'fpr_over, tpr_over, _ = roc_curve(y_test, props_oversample)\n', '\n', 'undersample_pipe_rf.fit(X_train, y_train)\n', 'props_undersample = undersample_pipe_rf.predict_proba(X_test)[:, 1]\n', 'fpr_under, tpr_under, _ = roc_curve(y_test, props_undersample)\n', '\n', 'rf = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n', 'props_original = rf.predict_proba(X_test)[:, 1]\n', 'fpr_org, tpr_org, _ = roc_curve(y_test, props_original)\n', '\n', 'plt.plot(fpr_org, tpr_org, label=""original"")\n', 'plt.plot(fpr_over, tpr_over, label=""oversample"")\n', 'plt.plot(fpr_under, tpr_under, label=""undersample"")\n', 'plt.legend()\n', 'plt.xlabel(""FPR"")\n', 'plt.ylabel(""TPR"")\n', 'plt.title(""RF comparison"")']",MODEL,EVALUATE
1517,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,28,"['from sklearn.linear_model import LogisticRegression\n', ""scores = cross_val_score(LogisticRegression(class_weight='balanced'),\n"", ""                         X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(scores.mean())']",MODEL,EVALUATE
1518,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,29,"['from sklearn.ensemble import RandomForestClassifier\n', ""scores = cross_val_score(RandomForestClassifier(n_estimators=100, class_weight='balanced'),\n"", ""                         X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1520,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,32,"[""scores = cross_val_score(resampled_rf, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1522,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,34,"[""resampled_tree_test = make_resampled_ensemble(DecisionTreeClassifier(max_features='auto'))\n"", '\n', ""scores = cross_val_score(resampled_tree_test, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1523,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,35,"['resampled_lr = make_resampled_ensemble(LogisticRegression())\n', '\n', ""scores = cross_val_score(resampled_lr, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1526,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,39,"['enn_pipe = make_imb_pipeline(EditedNearestNeighbours(n_neighbors=5),\n', '                             LogisticRegression())\n', ""scores = cross_val_score(enn_pipe, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1527,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,40,"['enn_pipe_rf = make_imb_pipeline(EditedNearestNeighbours(n_neighbors=5),\n', '                                RandomForestClassifier(n_estimators=100))\n', ""scores = cross_val_score(enn_pipe_rf, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1536,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,50,['idx_resampled'],EXPLORE,EVALUATE
1538,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,52,"['tl = TomekLinks(return_indices=True)\n', 'X_resampled, y_resampled, idx_resampled = tl.fit_sample(X_syn, y_syn)\n', '\n', 'np.bincount(y_resampled)']",WRANGLE,MODEL
1544,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,58,"['cnn_pipe = make_imb_pipeline(CondensedNearestNeighbour(), LogisticRegression())\n', ""scores = cross_val_score(cnn_pipe, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1545,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,59,"['cnn_pipe = make_imb_pipeline(CondensedNearestNeighbour(),\n', '                               RandomForestClassifier(n_estimators=100))\n', ""scores = cross_val_score(cnn_pipe, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1550,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,66,"['smote_pipe = make_imb_pipeline(SMOTE(), LogisticRegression())\n', ""scores = cross_val_score(smote_pipe, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1551,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,67,"['smote_pipe_rf = make_imb_pipeline(SMOTE(), RandomForestClassifier(n_estimators=100))\n', ""scores = cross_val_score(smote_pipe_rf, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1557,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,73,"['from imblearn.combine import SMOTEENN, SMOTETomek\n', 'smoteenn_pipe_rf = make_imb_pipeline(SMOTEENN(smote=SMOTE(k_neighbors=11)), RandomForestClassifier(n_estimators=100))\n', ""scores = cross_val_score(smoteenn_pipe_rf, X_train, y_train, cv=10, scoring='roc_auc')\n"", 'print(np.mean(scores))']",MODEL,EVALUATE
1559,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,75,['np.bincount(y_train_smote11)'],EXPLORE,EVALUATE
1560,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215903.ipynb,1215903,76,['np.bincount(y_train_smoteenn)'],EXPLORE,EVALUATE
678,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,1,"['from imp import reload\n', 'import pydss\n', 'import time\n', 'pydss = reload(pydss)\n', '\n', 't_0 = time.time()\n', ""sys1 = pydss.pydss('cigre_lv_isolated.json')\n"", 'sys1.pf_eval()\n', ""print('time: {:2.3f}'.format(time.time() - t_0))\n"", 's_0 = sys1.pq_3pn']",MODEL,WRANGLE
679,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,2,"['sys1.pq_3pn = s_0\n', 'sys1.pf_eval()\n', 'sys1.get_v()\n', 'sys1.get_i()']",EVALUATE,WRANGLE
683,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,7,"[""sys1 = pydss.pydss('cigre_lv_isolated.json')\n"", 't0 = time.time()\n', 'sys1.N_steps = 5000\n', 'sys1.pf_eval()\n', 'sys1.run_eval()\n', ""print('time: {:f}'.format(time.time()-t0))""]",MODEL,EVALUATE
684,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,8,"['fig = figure(width=600, height=300)\n', ""T = sys1.params_run[0]['T']\n"", ""I = sys1.params_run[0]['out_cplx_i']\n"", 'N = sys1.params_run[0].N_outs\n', '#X = sys1.system.X\n', 'decim = 1\n', 'for it in range(8):\n', ""    colors = ['red','green','blue','black']\n"", '    fig.line(T[0:N:decim,0],np.abs(I[0:N,it]), color=colors[it%4], line_width=1)\n', 'show(fig)']",EVALUATE,EXPLORE
685,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,9,"['fig = figure(width=600, height=300)\n', ""T = sys1.params_run[0]['T']\n"", ""T_j_igbt = sys1.params_run[0]['T_j_igbt_abcn']+25\n"", ""T_sink_1 = sys1.params_run[0]['T_sink']+25\n"", ""T_sink_2 = sys1.params_run[0]['T_sink']+25\n"", 'N = sys1.params_run[0].N_outs\n', '#X = sys1.system.X\n', 'decim = 1\n', ""colors = ['red','green','blue','black']\n"", 'for it in range(3):    \n', ""    fig.line(T[0:N:decim,0],T_j_igbt[0:N,it+0].real, color='red', line_width=1)\n"", 'for it in range(3):    \n', ""    fig.line(T[0:N:decim,0],T_j_igbt[0:N,it+4].real, color='green', line_width=1)\n"", '    \n', ""fig.line(T[0:N:decim,0],T_sink_1[0:N,0].real, color='red', line_width=1)\n"", ""fig.line(T[0:N:decim,0],T_sink_2[0:N,0].real, color='green', line_width=1)\n"", 'show(fig)']",EVALUATE,EXPLORE
686,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,10,"['fig = figure(width=600, height=300)\n', ""T = sys1.params_run[0]['T']\n"", ""V = sys1.params_run[0]['out_cplx_v']\n"", 'N = sys1.params_run[0].N_outs\n', '#X = sys1.system.X\n', 'decim = 1\n', 'for it in range(8):\n', ""    colors = ['red','green','blue','black']\n"", '    fig.line(T[0:N:decim,0],np.abs(V[0:N:decim,it]), color=colors[it%4], line_width=1)\n', 'show(fig)']",EVALUATE,EXPLORE
687,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,11,"['fig = figure(width=600, height=300)\n', ""T = sys1.params_run[0]['T']\n"", ""V = sys1.params_run[0]['out_cplx_v']\n"", 'N = sys1.params_run[0].N_outs\n', '#X = sys1.system.X\n', 'decim = 1\n', 'for it in range(8):\n', ""    colors = ['red','green','blue','black']\n"", '    fig.line(T[0:N:decim,0],np.angle(V[0:N:decim,it]), color=colors[it%4], line_width=1)\n', 'show(fig)']",EVALUATE,EXPLORE
688,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,12,"['fig = figure(width=600, height=300)\n', 'decim = 1\n', 'for it in range(72):\n', ""    colors = ['red','green','blue','black']\n"", '    \n', '    fig.line(T[0:N:decim,0],np.abs(V[0:N:decim,it]), color=colors[it%4], line_width=1)\n', ""fig.line(T[[0,N],0],[231*1.1,231*1.1],  line_width=3, color='red')\n"", ""fig.line(T[[0,N],0],[231*1.05,231*1.05],  line_width=2, color='red')\n"", ""fig.line(T[[0,N],0],[231*0.95,231*0.95],  line_width=2, color='blue')\n"", ""fig.line(T[[0,N],0],[231*0.9,231*0.9],  line_width=3, color='blue')\n"", ""fig.line(T[[0,N],0],[231,231],  line_width=2, color='green')\n"", 'show(fig)']",EVALUATE,EXPLORE
690,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,14,"['figp = figure(width=600, height=300)\n', 'for it in range(6):\n', ""    colors = ['red','green','blue','black']\n"", '    P=sys1.system.out_cplx[it+72,:].real/1000\n', '    figp.line(T[:,0],P, color=colors[it%3], line_width=1)\n', 'show(figp)']",EVALUATE,EXPLORE
691,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,16,"['fig = figure(width=600, height=300)\n', 'fig.line(T[:,0],sys1.system.out[24,:], line_width=1)\n', 'show(fig)']",EVALUATE,EXPLORE
692,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,17,"['import numpy as np\n', 'from bokeh.io import output_notebook, show\n', 'from bokeh.models import ColumnDataSource\n', 'from bokeh.io import push_notebook\n', 'from bokeh.layouts import gridplot\n', 'from bokeh.plotting import figure\n', 'from bokeh.charts import Bar \n', 'import pandas as pd\n', 'output_notebook()\n', '\n', 'sys1.pq_3pn = s_0\n', 'sys1.pf_eval()\n', 'sys1.get_v()\n', 'sys1.get_i()\n', '\n', 'plt_voltages = figure(width=600, height=300,x_range = [50,-300], y_range = [180,250])\n', 'source = ColumnDataSource(sys1.bus_data)\n', 'cr = plt_voltages.circle(source=source, x=\'y\', y=\'v_an\', size=15, color=""red"", alpha=0.5)\n', 'plt_voltages.circle(source=source, x=\'y\', y=\'v_bn\', size=15, color=""green"", alpha=0.5)\n', 'plt_voltages.circle(source=source, x=\'y\', y=\'v_cn\', size=15, color=""blue"", alpha=0.5)\n', ""plt_voltages.line([-300, 50],[231*1.05,231*1.05], color='red', line_width=5)\n"", ""plt_voltages.line([-300, 50],[231*0.90,231*0.90], color='blue', line_width=5)\n"", '#plt_voltages.add_tools(HoverTool(renderers=[cr], tooltips=sys1.bus_tooltip))\n', '\n', 's = sys1.V_known * np.conj(sys1.I_unknown)\n', 'p = s.real/1000\n', '\n', ""d = {'bus':['N1']*4+['N2']*4,'node':['a','b','c','n']*2,'p':list(p[:,0]), 'pos':range(8)}\n"", '#source_powers = pd.DataFrame(d)\n', 'source_powers = ColumnDataSource(d)\n', '\n', '\n', ""#plt_powers = Bar(source_powers, values='p',label='bus',  group='node', \n"", '#        title=""Avg MPG by ORIGIN, stacked by CYL"") #  legend=\'top_left\'\n', ""plt_powers = figure(width=600, height=300, y_range = [-100,150], y_axis_label='Powers', x_axis_label='Pos (m)')\n"", 'plt_powers.circle(source=source_powers, x=\'pos\', y=\'p\', size=15, color=""green"", alpha=0.5)\n', '\n', 'def update(load_factor=1.0):\n', '\n', '    sys1.pq_3pn = load_factor*s_0\n', '\n', '    sys1.pf_eval()\n', '    sys1.get_v()\n', '    sys1.get_i()\n', '    sys1.bokeh_tools()\n', '    s = sys1.system.V_known * np.conj(sys1.system.I_unknown)\n', '    p = s.real/1000\n', '\n', '    source.data = sys1.bus_data\n', ""    source_powers.data= {'bus':['N1']*4+['N2']*4,'node':['a','b','c','n']*2,'p':list(p[:,0]), 'pos':range(8)}\n"", '\n', '    push_notebook()\n', '\n', 'p_grid = gridplot([[plt_voltages], [plt_powers]])\n', 'show(p_grid,notebook_handle=True)\n', '\n', '\n', '#show(p, notebook_handle=True)\n']",MODEL,EXPLORE
693,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1215918.ipynb,1215918,18,"['from ipywidgets import interact\n', 'interact(update, load_factor=(-1,1.2, 0.01))']",EVALUATE,EXPLORE
1483,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1218288.ipynb,1218288,4,"['# combine data into one DataFrame\n', ""data = pd.DataFrame({'VIX':vol.VIX,'VXV':vol.VXV,'VXX':vxx}).dropna()\n"", 'data.plot(logy=True) #使用对数坐标']",EXPLORE,WRANGLE
1484,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1218288.ipynb,1218288,5,"['# now we will create dataset X containing independant variables and dataset Y with next-day returns of VXX\n', '\n', 'k = 5 # lookback for rolling sum of the VXX returns\n', 'R = data.VXX.pct_change() # daily returns of VXX\n', '\n', ""X = pd.DataFrame({'a':data.VIX-data.VXV,'b':pd.rolling_sum(R,k)}) # independent variables dataset,with columns 'a' and 'b'\n"", 'Y= R.shift(-1) # next-day return\n', '\n', '# X and Y at this moment contain some rows with nan values. \n', '# These will cause problems later on as knn does not handle these well.\n', '# it is a good idea to clean up the missing values, this can be done on one line.\n', 'uniqueIdx = pd.concat((X,Y), axis=1).dropna().index # concatenate X and Y to a joint dataset, drop nans and get index\n', '\n', ""# reindex X and Y to rows that don't contain nans (removes some rows)\n"", 'X = X.reindex(uniqueIdx)\n', 'Y = Y.reindex(uniqueIdx)\n', '\n', ""X.plot(kind='scatter',x='a',y='b') # plot the data""]",EXPLORE,WRANGLE
1490,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1218288.ipynb,1218288,15,"['# calculate Sharpe ratios for different strategies and buy-and-hold\n', 'print(twp.sharpe(PNL.ix[-split:,:]))\n', 'print(twp.sharpe(-Y[-split:]))']",EXPLORE,EVALUATE
1201,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1221697.ipynb,1221697,4,"['""""""\n', '    encordData,decordData\n', '    firstNum:\n', '    goalNum:\n', '    commonLabel:共通名前\n', '""""""\n', 'def MakeGlaph(encordData,decordData,firstNum,goalNum,commonLabel):\n', '    eRow, eCol=encordData.shape\n', '    dRow, dCol=decordData.shape\n', '    \n', '    if eRow!=dRow | eCol!=dCol:\n', '        return -1\n', '    start = firstNum\n', '    goal=goalNum\n', ""    print str(start)+' is start '+str(goal)+' is goal'\n"", '    dataE = encordData.reshape(eRow*eCol,1)[start:goal]\n', '    dataD = decordData.reshape(dRow*dCol,1)[start:goal]\n', '    plt.clf() \n', '    plt.figure(figsize=(100,50))\n', '    plt.plot(range(0,len(dataE)),dataE,\'--r\',label=""raw data"")\n', '    plt.plot(range(0,len(dataD)),dataD,\'b\',label=""reconstruct"")\n', ""    plt.legend(loc='best')\n"", '    plt.title(str(start)+""-""+str(goal))\n', '    plt.savefig(commonLabel+\'-\'+str(start)+""-""+str(goal)+\'-\'+\'glaph\')\n', '    plt.clf()\n', '    plt.cla()\n', ""    plt.close('all')\n"", '    return 1']",WRANGLE,EXPLORE
1203,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1221697.ipynb,1221697,8,"['# define SaveFileName\n', '# 8, 4, 12, 16, 32, 64\n', '# 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n', 'def kld(p, q):\n', '    """"""Calculates Kullback–Leibler divergence""""""\n', '    p = np.array(p)\n', '    q = np.array(q)\n', '    return np.sum(p * np.log(p / q), axis=(p.ndim - 1))\n', ' \n', 'def jsd(p, q):\n', '    """"""Calculates Jensen-Shannon Divergence""""""\n', '    p = np.array(p)\n', '    q = np.array(q)\n', '    m = 0.5 * (p + q)\n', '    return 0.5 * kld(p, m) + 0.5 * kld(q, m)\n', '\n', 'AllEncoding_dim = [4, 8, 12, 16, 32, 64]  \n', ""window = processing.LoadDicDataFromFileNPZ(WindowDataPath+SensorName+'_'+Axis+'_train_edim=8.npz')\n"", 'rows,colms=window.shape\n', 'window = window.reshape(rows,colms/3,3)\n', 'fig = plt.figure(figsize=(30,15))\n', 'ax = fig.add_subplot(111)\n', '\n', 'AllWindow = [window[:,:,0],window[:,:,1],window[:,:,2]]\n', ""WindowName=['AccX','AccY','AccZ']\n"", '\n', 'for rawData,name in zip(AllWindow,WindowName):\n', '    for encoding_dim in AllEncoding_dim:\n', ""        SaveFileNameGraph=GlaphDataPath+ DataName+'_edim='+str(encoding_dim)+'-'+name\n"", '\n', ""        PowerDataE=processing.LoadDicDataFromFileNPZ(StudyDataPath+DataName+'_edim='+str(encoding_dim)+'-'+name+'-power-raw.npz')\n"", ""        PowerDataD=processing.LoadDicDataFromFileNPZ(StudyDataPath+DataName+'_edim='+str(encoding_dim)+'-'+name+'-power-restruct.npz')\n"", '        \n', '        # to distribution\n', '        DistributePowerDataE = np.array([])\n', '        DistributePowerDataD = np.array([])\n', '\n', '        for i in range( PowerDataE.shape[0] ):\n', '            DistributePowerDataE = np.append(DistributePowerDataE,PowerDataE[i]/( np.sum(PowerDataE[i]) ))\n', '        DistributePowerDataE = DistributePowerDataE.reshape(PowerDataE.shape)\n', '\n', '        for i in range( PowerDataD.shape[0] ):\n', '            DistributePowerDataD = np.append(DistributePowerDataD,PowerDataD[i]/( np.sum(PowerDataD[i]) ))\n', '        DistributePowerDataD = DistributePowerDataD.reshape(PowerDataD.shape)\n', '\n', ""        processing.SaveDicDataFromFileNPZ(StudyDataPath, DataName+'_edim='+str(encoding_dim)+'-'+name+'-power-distribution-raw',DistributePowerDataE)\n"", ""        processing.SaveDicDataFromFileNPZ(StudyDataPath, DataName+'_edim='+str(encoding_dim)+'-'+name+'-power-distribution-reconst',DistributePowerDataD)\n"", '        \n', '        ArrayKLD = np.array([])\n', '\n', '        # Kullback-leibler-divergence plot\n', '        for i in range(DistributePowerDataE.shape[0]):\n', '            ArrayKLD = np.append(ArrayKLD, kld(DistributePowerDataE[i],DistributePowerDataD[i]))\n', '\n', '        for i in range(width,DistributePowerDataE.shape[0],width):\n', '            start =i-width\n', '            goal=i\n', '\n', '            data = ArrayKLD[start:goal]\n', '\n', '            #dataE,dataDをPower\n', '            ax.plot(range(0,len(data)),data,\'g\',label=""kld data"")\n', ""            ax.legend(loc='best')\n"", '            ax.set_title(str(start)+""-""+str(goal))\n', '            ax.figure.savefig(SaveFileNameGraph+\'-\'+str(start)+""-""+str(goal)+\'-\'+\'-glaph\')\n', '            ax.clear()\n', ""            print 'Graph range='+str(start)+' : '+str(goal)\n"", '            time.sleep(3)']",EXPLORE,WRANGLE
1590,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1222945.ipynb,1222945,2,"['import pandas as pd\n', 'import os\n', '\n', ""data = pd.read_csv(os.path.join('..', '..', 'assets', 'dataset', 'ozone.csv'))""]",WRANGLE,IMPORT
1593,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1222945.ipynb,1222945,7,"['a = data.loc[0:1]\n', 'print a\n', 'print (""It looks like someone recording the stats for the day"")']",WRANGLE,EXPLORE
779,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1223629.ipynb,1223629,7,"['def sigmoid(z):  \n', '    return 1 / (1 + np.exp(-z))']",WRANGLE,MODEL
786,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1223629.ipynb,1223629,19,"['def predict(theta, X):  \n', '    probability = sigmoid(X * theta.T)\n', '    return [1 if x >= 0.5 else 0 for x in probability]\n', '\n', 'theta_min = np.matrix(result[0])  \n', 'predictions = predict(theta_min, X)  \n', 'correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]  \n', 'accuracy = (sum(map(int, correct)) % len(correct))  \n', ""print 'accuracy = {0}%'.format(accuracy)  ""]",MODEL,EVALUATE
791,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1223629.ipynb,1223629,29,"['def gradientReg(theta, X, y, learningRate):  \n', '    theta = np.matrix(theta)\n', '    X = np.matrix(X)\n', '    y = np.matrix(y)\n', '\n', '    parameters = int(theta.ravel().shape[1])\n', '    grad = np.zeros(parameters)\n', '\n', '    error = sigmoid(X * theta.T) - y\n', '\n', '    for i in range(parameters):\n', '        term = np.multiply(error, X[:,i])\n', '\n', '        if (i == 0):\n', '            grad[i] = np.sum(term) / len(X)\n', '        else:\n', '            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])\n', '\n', '    return grad']",WRANGLE,MODEL
792,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1223629.ipynb,1223629,30,"['# set X & y\n', 'cols = data2.shape[1]  \n', 'X2 = data2.iloc[:,1:cols]  \n', 'y2 = data2.iloc[:,0:1]\n', '\n', '# convert to numpy arrays and initalize the parameter array theta\n', 'X2 = np.array(X2.values)  \n', 'y2 = np.array(y2.values)  \n', 'theta2 = np.zeros(11)\n', '\n', 'learningRate = 1\n', '\n', 'costReg(theta2, X2, y2, learningRate)  ']",WRANGLE,MODEL
1698,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1227519.ipynb,1227519,4,"['vectorizer = CountVectorizer()\n', 'vectorizer.fit(train_data)\n', 'pickle.dump(vectorizer,open(""train_vectorizer.pickle"",""wb""))\n', 'train_mat = vectorizer.transform(train_data)\n', 'test_mat = vectorizer.transform(test_data)\n', 'print train_mat.shape\n', 'print test_mat.shape']",WRANGLE,MODEL
1699,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1227519.ipynb,1227519,6,"['tfidf = TfidfTransformer()\n', 'tfidf.fit(train_mat)\n', 'pickle.dump(tfidf,open(""train_tfidftransformer.pickle"",""wb""))\n', 'train_tfmat = tfidf.transform(train_mat)\n', 'print train_tfmat.shape\n', 'test_tfmat = tfidf.transform(test_mat)\n', 'print test_tfmat.shape']",WRANGLE,MODEL
556,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1228071.ipynb,1228071,10,"['active_titles = rating_by_title.index[rating_by_title >=250 ]\n', 'active_titles']",WRANGLE,EXPLORE
557,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1228071.ipynb,1228071,11,"['mean_ratings = mean_ratings.ix[active_titles]\n', 'mean_ratings']",WRANGLE,EXPLORE
1355,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1234234.ipynb,1234234,4,"['#Build connection with mysql.\n', 'sql_ip = ""127.0.0.1""\n', 'port = 3306\n', 'user = ""root""\n', 'passwd = ""hanjingfei007""\n', 'db = ""aminer_gai""\n', '\n', ""conn = MySQLdb.connect(host=sql_ip, user=user, port=port, passwd=passwd, db=db, charset='utf8')\n""]",IMPORT,WRANGLE
37,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1236355.ipynb,1236355,11,"['# what happens if the movie name is not recognized?\n', ""r = requests.get('http://www.omdbapi.com/?t=thebestmovieevermade&r=json&type=movie')\n"", 'print r.status_code\n', 'r.json()']",WRANGLE,EXPLORE
38,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1236355.ipynb,1236355,12,"['# another example\n', ""movie_title = 'finding dory'\n"", ""r = requests.get('http://www.omdbapi.com/?t='+movie_title+'&r=json&type=movie')\n"", 'print r.status_code\n', 'r.json()']",WRANGLE,EXPLORE
46,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1236355.ipynb,1236355,20,"['# save that list as a new column\n', ""top_movies['year'] = years\n"", 'top_movies']",WRANGLE,EXPLORE
49,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1236355.ipynb,1236355,23,"['# create a new column and set a default value\n', ""movies['yearsr'] = None\n"", 'movies.head()']",WRANGLE,EXPLORE
50,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1236355.ipynb,1236355,24,"[""# loc method allows you to access a DataFrame element by 'label'\n"", ""movies.loc[0, 'year'] = 1994\n"", 'movies.head()']",WRANGLE,EXPLORE
719,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1240128.ipynb,1240128,2,"['num_features = 300\n', ""model = gensim.models.Word2Vec.load_word2vec_format('/home/bahbbc/Documents/personality_1', encoding='utf8', unicode_errors='ignore')\n"", ""#model = Word2Vec.load_word2vec_format('/home/bahbbc/Documents/teste-personalidade/personality_1',binary=True)\n"", 'model.init_sims(replace=True)']",WRANGLE,MODEL
725,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1240128.ipynb,1240128,10,"['def predict(vectorizer, classifier, data):\n', ""    data_features = vectorizer.transform(data['formatted_text'])\n"", '    predictions = classifier.predict(data_features)\n', ""    target = data['openness_m']\n"", '    evaluate_prediction(predictions, target)']",MODEL,EVALUATE
193,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1242788.ipynb,1242788,8,"[""player_stats = df[['winner_name','w_ace', 'w_df', 'w_svpt','w_1stIn','w_1stWon','w_2ndWon','w_SvGms','w_bpFaced','w_bpSaved']].groupby(['winner_name']).filter(lambda x: len(x) > 100).groupby(['winner_name']).agg(['mean', 'count'])\n"", '\n', ""player_stats.loc['Andy Murray']['w_ace']""]",EXPLORE,WRANGLE
176,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1242794.ipynb,1242794,45,"['#data = [\n', '#    Histogram(\n', '#        x= abs(lm.predict(x_test) - y_test)\n', '#    )\n', '#]\n', '#iplot(data)\n', 'lm.predict(x_test) - y_test']",MODEL,EVALUATE
184,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1242794.ipynb,1242794,56,"['perm_data_15.ix[perm_data_15[\'CASE_STATUS\'].str.startswith(""C""), \'ACCEPTED_CLASS\'] = \'2\'\n', 'perm_data_15.ix[perm_data_15[\'CASE_STATUS\'].str.startswith(""D""), \'ACCEPTED_CLASS\'] = \'1\'']",WRANGLE,EXPLORE
747,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1245037.ipynb,1245037,2,"['#Connect to the remote database with paramaters provided\n', 'import pandas as pd\n', '#import psycopg2 \n', 'import sqlalchemy\n', '\n', '\n', ""engine = sqlalchemy.create_engine('postgresql://dsi_student:gastudents@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com:5432/northwind')\n"", '\n', '# This can work but sometime there are issues with the connection being specifically supported by psql\n', '# params = {\n', ""#   'dbname': 'northwind',\n"", ""#   'user': 'dsi_student',\n"", ""#   'password': 'gastudents',\n"", ""#   'host': 'dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com',\n"", ""#   'port': 5432\n"", '# }\n', '\n', '# conn = psycopg2.connect(**params)']",WRANGLE,IMPORT
1668,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1249011.ipynb,1249011,4,"[""BaseAlgs = [{'BaseAlg':'SimpleExponentialSmoothing', 'BaseAlgParams':{'alpha':0.1, 'AdaptationPeriod':10}},\n"", ""            {'BaseAlg':'AdaptiveExponentialSmoothing', 'BaseAlgParams':{'alpha':0.2,'gamma':0.01, 'AdaptationPeriod':10}}\n"", ""#                  {'BaseAlg':'AdaptiveSimpleExponentialSmoothing', 'BaseAlgParams':{'alpha':0.1, 'gamma':0.01}},\n"", ""#                 {'BaseAlg':'TheilWageSmoothing', 'BaseAlgParams':{'alpha':0.3, 'beta':0.9,'delta':0.9}}\n"", ']']",MODEL,WRANGLE
1669,http://quackerjack.cs.washington.edu:5000/annotatenb/nb_1249011.ipynb,1249011,5,"['for ba in range(len(BaseAlgs)):\n', '    print(BaseAlgs[ba][\'BaseAlg\']+""(x,h,""+""BaseAlgs[""+str(ba)+""][\'BaseAlgsParams\'])"")']",MODEL,EXPLORE
